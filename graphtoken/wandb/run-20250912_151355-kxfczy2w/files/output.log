= = = = = = = = = = = = = = = = = = = =
## Starting Time: 09-12 14:13:57
Namespace(dataset='huggingface', llm='Mistral-7B', llm_model_path='', seed=0, device='cuda:0', max_txt_length=512, max_ans_length=512, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_output_dim=4096, n_layers=2, gnn_type='SAGE', max_degree=300, num_epochs=2, batch_size=6, eval_batch_size=6, patience=2, lr=1e-05, wd=0.05, output_dir='output', grad_steps=4, name='diffpool')

[Training Data] # Chain Samples 1536 (51.20)
[Data Split] # Train 3000  # Test 500
# Train 2400   # Val 600   # Test 500
Loading checkpoint shards: 100%|█████████████████████████████████| 3/3 [00:01<00:00,  2.12it/s]
Finish loading pre-trained Mistral-7B model!
Trainable params 8466467 || all params 7250198563 || trainable% 0.11678
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(row_sum, -0.5).flatten()
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:19: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:653.)
  return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))
  0%|                                                                  | 0/800 [00:00<?, ?it/s]W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0] Graph break from `Tensor.item()`, consider setting:
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]     torch._dynamo.config.capture_scalar_outputs = True
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0] or:
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0] to include these operations in the captured graph.
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0] Graph break: from user code at:
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]   File "/nas/home/ktshim/tool/pool/graphtoken/glm_diffpool.py", line 62, in encode_task_graph
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]     node_embeds = self.graph_tokenizer(task_graph.x, task_graph.edge_index)
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]   File "/nas/home/ktshim/tool/pool/graphtoken/gnn_diffpool.py", line 150, in forward
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]     x_embed_dense, mask = to_dense_batch(x_embed, batch)
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]   File "/home/ktshim/.local/lib/python3.12/site-packages/torch_geometric/experimental.py", line 117, in wrapper
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]     return func(*args, **kwargs)
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]   File "/home/ktshim/.local/lib/python3.12/site-packages/torch_geometric/utils/_to_dense_batch.py", line 104, in to_dense_batch
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]     batch_size = int(batch.max()) + 1
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]
W0912 15:14:05.236000 3815274 torch/_dynamo/variables/tensor.py:1047] [7/0]
/nas/home/ktshim/tool/pool/graphtoken/glm_diffpool.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|                                                        | 1/800 [00:44<9:48:25, 44.19s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_diffpool.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  0%|▏                                                      | 2/800 [01:40<11:20:48, 51.19s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_diffpool.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Traceback (most recent call last):
  File "/nas/home/ktshim/tool/pool/graphtoken/main.py", line 139, in <module>
    loss.backward()
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2259, in backward
    return impl_fn()
           ^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2245, in impl_fn
    out = CompiledFunction._backward_impl(ctx, all_args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2376, in _backward_impl
    out = call_func_at_runtime_with_args(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 584, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_inductor/utils.py", line 2716, in run
    out = model(new_inputs)
          ^^^^^^^^^^^^^^^^^
  File "/tmp/torchinductor_ktshim/yc/cycq6egvz2bdyvri7bd2d725gprtkvhzkjapiheevx5usxdgety6.py", line 2068, in call
    buf41 = torch.ops.aten._scaled_dot_product_efficient_attention_backward.default(reinterpret_tensor(buf39, (6, 32, s77, 128), (4096*s77, 128, 4096, 1), 0), buf9, reinterpret_tensor(buf8, (6, 32, s77, 128), (4096*s77, 128*s77, 128, 1), 0), reinterpret_tensor(buf10, (6, 32, s77, 128), (4096*s77, 128*s77, 128, 1), 0), buf40, buf13, buf14, buf15, buf16, 0.0, [True, True, True, False], scale=0.08838834764831845)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_ops.py", line 829, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: attn_bias is not correctly aligned (strideH) .attn_bias.stride(1) = 480249, and should be a multiple of 4.
