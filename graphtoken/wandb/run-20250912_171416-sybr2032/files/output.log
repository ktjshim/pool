= = = = = = = = = = = = = = = = = = = =
## Starting Time: 09-12 16:14:17
Namespace(dataset='ultratool', llm='Mistral-7B', llm_model_path='', seed=0, device='cuda:0', max_txt_length=512, max_ans_length=512, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_output_dim=4096, n_layers=2, gnn_type='SAGE', max_degree=300, max_nodes=23, num_epochs=2, batch_size=6, eval_batch_size=6, patience=2, lr=1e-05, wd=0.05, output_dir='output', grad_steps=4, name='graph')

[Training Data] # Chain Samples 3000 (100.00)
[Data Split] # Train 3000  # Test 500
# Train 2400   # Val 600   # Test 500
Loading checkpoint shards: 100%|██| 3/3 [00:01<00:00,  2.11it/s]
Finish loading pre-trained Mistral-7B model!
Trainable params 10492928 || all params 7252225024 || trainable% 0.14469
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(row_sum, -0.5).flatten()
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:19: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:653.)
  return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))
  0%|                                   | 0/800 [00:00<?, ?it/s]/home/ktshim/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/nas/home/ktshim/tool/pool/graphtoken/glm_graph.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|                         | 1/800 [00:36<8:10:00, 36.80s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_graph.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Traceback (most recent call last):
  File "/nas/home/ktshim/tool/pool/graphtoken/main.py", line 140, in <module>
    loss, logits = model(batch, task_graph)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 749, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1871, in _call_user_compiler
    raise BackendCompilerFailed(
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py", line 1846, in _call_user_compiler
    compiled_fn = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py", line 150, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/__init__.py", line 2380, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py", line 2418, in compile_fx
    return aot_autograd(
           ^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/backends/common.py", line 109, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1199, in aot_module_simplified
    compiled_fn = AOTAutogradCache.load(
                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py", line 1140, in load
    compiled_fn = dispatch_and_compile()
                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1184, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 576, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 836, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
                               ^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 1262, in aot_dispatch_autograd
    fx_g, joint_inputs, maybe_subclass_meta = aot_dispatch_autograd_graph(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py", line 318, in aot_dispatch_autograd_graph
    fx_g = _create_graph(joint_fn_to_trace, updated_joint_inputs, aot_config=aot_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py", line 55, in _create_graph
    fx_g = make_fx(
           ^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 2250, in trace
    return self._trace_inner(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 2221, in _trace_inner
    t = dispatch_trace(
        ^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py", line 850, in trace
    (self.create_arg(fn(*args)),),
                     ^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py", line 703, in flatten_fn
    tree_out = root_fn(*tree_args)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
          ^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 720, in inner_fn
    outs = fn(*args)
           ^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 671, in joint_helper
    return _functionalized_f_helper(primals, tangents)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 419, in _functionalized_f_helper
    f_outs = fn(*f_args)
             ^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 286, in inner_fn_with_anomaly
    return inner_fn(*args)
           ^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 197, in inner_fn
    outs, tangent_mask = fn(*primals)
                         ^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 118, in inner_fn
    outs = fn(*args_maybe_cloned)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py", line 902, in functional_call
    out = PropagateUnbackedSymInts(mod).run(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py", line 173, in run
    self.env[node] = self.run_node(node)
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py", line 7858, in run_node
    result = super().run_node(n)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py", line 242, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py", line 322, in call_function
    return target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_higher_order_ops/wrap.py", line 283, in __call__
    return Interpreter(gmod).run(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py", line 173, in run
    self.env[node] = self.run_node(node)
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py", line 242, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py", line 322, in call_function
    return target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 1360, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", line 307, in _fn
    result = fn(*args, is_out=(out is not None), **kwargs)  # type: ignore[arg-type]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_decomp/decompositions.py", line 4463, in matmul
    output = torch.ops.aten._unsafe_view(t1_folded.mm(t2), output_shape)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_ops.py", line 1243, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py", line 511, in __torch_dispatch__
    outs_unwrapped = func._op_dk(
                     ^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/utils/_stats.py", line 28, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 787, in proxy_call
    r = maybe_handle_decomp(proxy_mode, func, args, kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py", line 2390, in maybe_handle_decomp
    out = CURRENT_DECOMPOSITION_TABLE[op](*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", line 309, in _fn
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_decomp/decompositions.py", line 3887, in _reshape_alias
    return aten.view(x, shape)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/_ops.py", line 1243, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
RuntimeError: Overloaded torch operator invoked from Python failed to match any schema:
aten::view() Expected a value of type 'List[int]' for argument 'size' but instead found type 'list'.
Position: 1
Value: [6, s77, 4096]
Declaration: aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)
 Python error details: KeyboardInterrupt: <EMPTY MESSAGE>

At:
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/sym_node.py(222): maybe_as_int
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_ops.py(1243): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_decomp/decompositions.py(3887): _reshape_alias
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_prims_common/wrappers.py(309): _fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(2390): maybe_handle_decomp
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(787): proxy_call
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(1462): __torch_dispatch__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/utils/_stats.py(28): wrapper
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py(511): __torch_dispatch__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_ops.py(1243): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_decomp/decompositions.py(4463): matmul
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_prims_common/wrappers.py(307): _fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(1360): __torch_function__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py(322): call_function
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py(242): run_node
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py(173): run
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_higher_order_ops/wrap.py(283): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py(322): call_function
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py(242): run_node
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/symbolic_shapes.py(7858): run_node
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/interpreter.py(173): run
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(902): functional_call
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(118): inner_fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(197): inner_fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(286): inner_fn_with_anomaly
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(419): _functionalized_f_helper
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(671): joint_helper
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py(720): inner_fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(1312): wrapped
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py(703): flatten_fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py(850): trace
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py(929): _fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(1254): dispatch_trace
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py(929): _fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_compile.py(53): inner
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(2221): _trace_inner
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(2250): trace
  /home/ktshim/.local/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py(2318): wrapped
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py(55): _create_graph
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py(318): aot_dispatch_autograd_graph
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py(1262): aot_dispatch_autograd
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py(836): _create_aot_dispatcher_function
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py(576): create_aot_dispatcher_function
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py(1184): dispatch_and_compile
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py(1140): load
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py(1199): aot_module_simplified
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/backends/common.py(109): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_inductor/compile_fx.py(2418): compile_fx
  /home/ktshim/.local/lib/python3.12/site-packages/torch/__init__.py(2380): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py(150): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py(1846): _call_user_compiler
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py(1811): call_user_compiler
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py(1696): compile_and_call_fx_graph
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/output_graph.py(1422): compile_subgraph
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py(3653): _return
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py(3672): RETURN_VALUE
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py(1267): step
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py(1363): run
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py(3497): run
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(753): transform
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(267): _fn
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py(1424): transform_code_object
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(832): _compile_inner
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(793): compile_inner
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_utils_internal.py(97): wrapper_function
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(1111): _compile
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(629): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(1272): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py(1495): __call__
  /home/ktshim/.local/lib/python3.12/site-packages/transformers/utils/generic.py(1005): wrapper
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1784): _call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1773): _wrapped_call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py(434): forward
  /home/ktshim/.local/lib/python3.12/site-packages/transformers/utils/generic.py(959): wrapper
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1784): _call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1773): _wrapped_call_impl
  /nas/home/ktshim/tool/pool/graphtoken/glm_graph.py(127): torch_dynamo_resume_in_forward_at_90
  /nas/home/ktshim/tool/pool/graphtoken/glm_graph.py(90): torch_dynamo_resume_in_forward_at_89
  /nas/home/ktshim/tool/pool/graphtoken/glm_graph.py(89): torch_dynamo_resume_in_forward_at_88
  /nas/home/ktshim/tool/pool/graphtoken/glm_graph.py(88): torch_dynamo_resume_in_forward_at_85
  /nas/home/ktshim/tool/pool/graphtoken/glm_graph.py(85): torch_dynamo_resume_in_forward_at_84
  /nas/home/ktshim/tool/pool/graphtoken/glm_graph.py(84): forward
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1784): _call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1773): _wrapped_call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py(736): compile_wrapper
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1784): _call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/torch/nn/modules/module.py(1773): _wrapped_call_impl
  /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py(375): __call__
  /nas/home/ktshim/tool/pool/graphtoken/main.py(140): <module>


aten::view() Expected a value of type 'int' for argument 'dtype' but instead found type 'list'.
Position: 1
Value: [6, s77, 4096]
Declaration: aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
Cast error details: Unable to cast Python instance of type <class 'list'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)



While executing %down_proj : [num_users=1] = call_function[target=torch._C._nn.linear](args = (%mul_8, %l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, None), kwargs = {})
GraphModule: class GraphModule(torch.nn.Module):
    def forward(self, s77: "Sym(s77)", hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11.to(torch.float32)

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
        variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
        rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
        hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12.to(torch.float32);  hidden_states_12 = None
        hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = to_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
        size = hidden_states_13.size()
        getitem = size[0];  getitem = None
        getitem_1: "Sym(s77)" = size[1]
        getitem_2 = size[2];  size = getitem_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
        query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
        key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_13 = l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
        value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        size_1 = query_states.size()
        getitem_3 = size_1[0];  getitem_3 = None
        getitem_4 = size_1[1];  getitem_4 = None
        getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
        getitem_6 = size_1[3];  size_1 = getitem_6 = None
        x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        size_2 = query_states.size()
        getitem_8 = size_2[0];  getitem_8 = None
        getitem_9 = size_2[1];  getitem_9 = None
        getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
        getitem_11 = size_2[3];  size_2 = getitem_11 = None
        x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
        cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
        q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        size_3 = key_states.size()
        getitem_13 = size_3[0];  getitem_13 = None
        getitem_14 = size_3[1];  getitem_14 = None
        getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
        getitem_16 = size_3[3];  size_3 = getitem_16 = None
        x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        size_4 = key_states.size()
        getitem_18 = size_4[0];  getitem_18 = None
        getitem_19 = size_4[1];  getitem_19 = None
        getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
        getitem_21 = size_4[3];  size_4 = getitem_21 = None
        x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
        cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
        size_5 = k_embed.size()
        getitem_23 = size_5[0];  getitem_23 = None
        getitem_24 = size_5[1];  getitem_24 = None
        getitem_25: "Sym(s77)" = size_5[2]
        getitem_26 = size_5[3];  size_5 = getitem_26 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
        hidden_states_14: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_14.reshape(6, 32, getitem_25, 128);  hidden_states_14 = getitem_25 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
        size_6 = value_states.size()
        getitem_28 = size_6[0];  getitem_28 = None
        getitem_29 = size_6[1];  getitem_29 = None
        getitem_30: "Sym(s77)" = size_6[2]
        getitem_31 = size_6[3];  size_6 = getitem_31 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
        getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
        hidden_states_15: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
        value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_15.reshape(6, 32, getitem_30, 128);  hidden_states_15 = getitem_30 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
        size_7 = key.size()
        getitem_33 = size_7[0];  getitem_33 = None
        getitem_34 = size_7[1];  getitem_34 = None
        getitem_35: "Sym(s77)" = size_7[2]
        getitem_36 = size_7[3];  size_7 = getitem_36 = None
        attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
        query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
        key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
        value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
        size_8 = query.size()
        getitem_38 = size_8[0];  getitem_38 = None
        getitem_39 = size_8[1];  getitem_39 = None
        getitem_40: "Sym(s77)" = size_8[2]
        getitem_41 = size_8[3];  size_8 = getitem_41 = None
        gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
        attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
        attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
        attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
        attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
        hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11 + attn_output_3;  hidden_states_11 = attn_output_3 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16.to(torch.float32)

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17.pow(2)
        variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
        rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
        hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17 * rsqrt_1;  hidden_states_17 = rsqrt_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18.to(torch.float32);  hidden_states_18 = None
        hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_ = None
        silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
        linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_19 = l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_ = None
        mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
        down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
        hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16 + down_proj;  hidden_states_16 = down_proj = None
        return (hidden_states_20,)


Original traceback:
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/utils/generic.py", line 1083, in torch_dynamo_resume_in_wrapper_at_1005
    outputs = func(self, *args, **kwargs)
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py", line 364, in forward
    hidden_states = decoder_layer(
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/modeling_layers.py", line 92, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py", line 243, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py", line 46, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))


While executing %tag_activation_checkpoint_12 : [num_users=1] = call_function[target=torch.ops.higher_order.tag_activation_checkpoint](args = (%wrap_body_12, %s77, %hidden_states_11, %l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_, %l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_, %l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_, %l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_, %cos_2, %sin_2, %causal_mask, %l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, %l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_, %l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_, %l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_, %l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_), kwargs = {use_reentrant: True})
GraphModule: class GraphModule(torch.nn.Module):
    def forward(self, s77: "Sym(s77)", L_kwargs_inputs_embeds_: "f32[6, s77, 4096][4096*s77, 4096, 1]", s16: "Sym(s16)", L_kwargs_attention_mask_: "i64[6, s16][s16, 1]", L_self_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]", L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]", L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]", L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", L_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameter
        l_kwargs_inputs_embeds_ = L_kwargs_inputs_embeds_
        l_kwargs_attention_mask_ = L_kwargs_attention_mask_
        l_self_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_norm_parameters_weight_ = L_self_modules_norm_parameters_weight_

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:343 in forward, code: cache_position = torch.arange(
        cache_position: "i64[s77][1]" = torch.arange(0, s77, device = device(type='cuda', index=0))

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:348 in forward, code: position_ids = cache_position.unsqueeze(0)
        position_ids: "i64[1, s77][s77, 1]" = cache_position.unsqueeze(0)

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/masking_utils.py:705 in _preprocess_mask_arguments, code: attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)
        attention_mask: "b8[6, s16][s16, 1]" = l_kwargs_attention_mask_.to(device = device(type='cuda', index=0), dtype = torch.bool);  l_kwargs_attention_mask_ = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/masking_utils.py:362 in sdpa_mask_recent_torch, code: kv_arange = torch.arange(kv_length, device=cache_position.device)
        kv_arange: "i64[s77][1]" = torch.arange(s77, device = device(type='cuda', index=0))

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/masking_utils.py:363 in sdpa_mask_recent_torch, code: kv_arange += kv_offset
        kv_arange += 0;  kv_arange_1: "i64[s77][1]" = kv_arange;  kv_arange = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/masking_utils.py:369 in sdpa_mask_recent_torch, code: batch_arange = torch.arange(batch_size, device=cache_position.device)
        batch_arange: "i64[6][1]" = torch.arange(6, device = device(type='cuda', index=0))

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/masking_utils.py:370 in sdpa_mask_recent_torch, code: head_arange = torch.arange(1, device=cache_position.device)
        head_arange: "i64[1][1]" = torch.arange(1, device = device(type='cuda', index=0))

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:313 in vmap_impl, code: lazy_load_decompositions()
        lazy_load_decompositions = torch._functorch.vmap.lazy_load_decompositions();  lazy_load_decompositions = None

        # No stacktrace found for following nodes
        _vmap_increment_nesting = torch._C._functorch._vmap_increment_nesting(6, 'error');  _vmap_increment_nesting = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:159 in _create_batched_inputs, code: arg if in_dim is None else _add_batch_dim(arg, in_dim, vmap_level)
        child: "i64[][]" = torch._C._functorch._add_batch_dim(batch_arange, 0, 1);  batch_arange = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:313 in vmap_impl, code: lazy_load_decompositions()
        lazy_load_decompositions_1 = torch._functorch.vmap.lazy_load_decompositions();  lazy_load_decompositions_1 = None

        # No stacktrace found for following nodes
        _vmap_increment_nesting_1 = torch._C._functorch._vmap_increment_nesting(1, 'error');  _vmap_increment_nesting_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:159 in _create_batched_inputs, code: arg if in_dim is None else _add_batch_dim(arg, in_dim, vmap_level)
        child_1: "i64[][]" = torch._C._functorch._add_batch_dim(head_arange, 0, 2);  head_arange = child_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:313 in vmap_impl, code: lazy_load_decompositions()
        lazy_load_decompositions_2 = torch._functorch.vmap.lazy_load_decompositions();  lazy_load_decompositions_2 = None

        # No stacktrace found for following nodes
        _vmap_increment_nesting_2 = torch._C._functorch._vmap_increment_nesting(s77, 'error');  _vmap_increment_nesting_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:159 in _create_batched_inputs, code: arg if in_dim is None else _add_batch_dim(arg, in_dim, vmap_level)
        child_2: "i64[][]" = torch._C._functorch._add_batch_dim(cache_position, 0, 3);  cache_position = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:313 in vmap_impl, code: lazy_load_decompositions()
        lazy_load_decompositions_3 = torch._functorch.vmap.lazy_load_decompositions();  lazy_load_decompositions_3 = None

        # No stacktrace found for following nodes
        _vmap_increment_nesting_3 = torch._C._functorch._vmap_increment_nesting(s77, 'error');  _vmap_increment_nesting_3 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:159 in _create_batched_inputs, code: arg if in_dim is None else _add_batch_dim(arg, in_dim, vmap_level)
        child_3: "i64[][]" = torch._C._functorch._add_batch_dim(kv_arange_1, 0, 4);  kv_arange_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142 in __torch_function__, code: return func(*args, **(kwargs or {}))
        result: "b8[][]" = child_2.new_ones((), dtype = torch.bool)

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142 in __torch_function__, code: return func(*args, **(kwargs or {}))
        le: "b8[][]" = child_3.le(child_2);  child_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142 in __torch_function__, code: return func(*args, **(kwargs or {}))
        to_1: "b8[][]" = le.to(device(type='cuda', index=0));  le = None
        result_1: "b8[][]" = result.__and__(to_1);  result = to_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:100 in forward, code: return torch.ops.aten.index(x, indices)
        index: "b8[][]" = torch.ops.aten.index(attention_mask, [child, child_3]);  attention_mask = child = child_3 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142 in __torch_function__, code: return func(*args, **(kwargs or {}))
        to_2: "b8[][]" = index.to(device(type='cuda', index=0));  index = None
        result_2: "b8[][]" = result_1.__and__(to_2);  result_1 = to_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:184 in _maybe_remove_batch_dim, code: return _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
        batched_outputs: "b8[s77][1]" = torch._C._functorch._remove_batch_dim(result_2, 4, s77, 0);  result_2 = None

        # No stacktrace found for following nodes
        _vmap_decrement_nesting = torch._C._functorch._vmap_decrement_nesting();  _vmap_decrement_nesting = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:184 in _maybe_remove_batch_dim, code: return _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
        batched_outputs_1: "b8[s77, s77][s77, 1]" = torch._C._functorch._remove_batch_dim(batched_outputs, 3, s77, 0);  batched_outputs = None

        # No stacktrace found for following nodes
        _vmap_decrement_nesting_1 = torch._C._functorch._vmap_decrement_nesting();  _vmap_decrement_nesting_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:184 in _maybe_remove_batch_dim, code: return _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
        batched_outputs_2: "b8[1, s77, s77][s77**2, s77, 1]" = torch._C._functorch._remove_batch_dim(batched_outputs_1, 2, 1, 0);  batched_outputs_1 = None

        # No stacktrace found for following nodes
        _vmap_decrement_nesting_2 = torch._C._functorch._vmap_decrement_nesting();  _vmap_decrement_nesting_2 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_functorch/vmap.py:184 in _maybe_remove_batch_dim, code: return _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
        causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = torch._C._functorch._remove_batch_dim(batched_outputs_2, 1, 6, 0);  batched_outputs_2 = None

        # No stacktrace found for following nodes
        _vmap_decrement_nesting_3 = torch._C._functorch._vmap_decrement_nesting();  _vmap_decrement_nesting_3 = None
        _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:288 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem_16: "f32[1, 64, 1][64, 1, 1]" = l_self_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]" = getitem_16.float();  getitem_16 = None
        expand: "f32[1, 64, 1][64, 1, 1]" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]" = expand.to(device(type='cuda', index=0));  expand = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:289 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_19: "i64[1, 1, s77][s77, s77, 1]" = position_ids[(slice(None, None, None), None, slice(None, None, None))];  position_ids = None
        position_ids_expanded: "f32[1, 1, s77][s77, s77, 1]" = getitem_19.float();  getitem_19 = None

        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:293 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, s77][s77, s77, 1]" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, s77][64*s77, s77, 1]" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, s77, 64][64*s77, 1, s77]" = matmul.transpose(1, 2);  matmul = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:294 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, s77, 128][128*s77, 128, 1]" = torch.cat((freqs, freqs), dim = -1);  freqs = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:295 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, s77, 128][128*s77, 128, 1]" = emb.cos()
        cos_1: "f32[1, s77, 128][128*s77, 128, 1]" = cos * 1.0;  cos = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:296 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, s77, 128][128*s77, 128, 1]" = emb.sin();  emb = None
        sin_1: "f32[1, s77, 128][128*s77, 128, 1]" = sin * 1.0;  sin = None

        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:298 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "f32[1, s77, 128][128*s77, 128, 1]" = cos_1.to(dtype = torch.float32);  cos_1 = None
        sin_2: "f32[1, s77, 128][128*s77, 128, 1]" = sin_1.to(dtype = torch.float32);  sin_1 = None

        # No stacktrace found for following nodes
        _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/torch/_dynamo/polyfills/__init__.py:209 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/modeling_layers.py:92 in __call__, code: return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
        wrap_body_0 = self.wrap_body_0
        tag_activation_checkpoint = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_0, s77, l_kwargs_inputs_embeds_, l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_0 = l_kwargs_inputs_embeds_ = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint[0];  tag_activation_checkpoint = None
        wrap_body_1 = self.wrap_body_1
        tag_activation_checkpoint_1 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_1, s77, hidden_states, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_1 = hidden_states = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_1[0];  tag_activation_checkpoint_1 = None
        wrap_body_2 = self.wrap_body_2
        tag_activation_checkpoint_2 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_2, s77, hidden_states_1, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_2 = hidden_states_1 = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_2[0];  tag_activation_checkpoint_2 = None
        wrap_body_3 = self.wrap_body_3
        tag_activation_checkpoint_3 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_3, s77, hidden_states_2, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_3 = hidden_states_2 = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_3[0];  tag_activation_checkpoint_3 = None
        wrap_body_4 = self.wrap_body_4
        tag_activation_checkpoint_4 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_4, s77, hidden_states_3, l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_4 = hidden_states_3 = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_4: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_4[0];  tag_activation_checkpoint_4 = None
        wrap_body_5 = self.wrap_body_5
        tag_activation_checkpoint_5 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_5, s77, hidden_states_4, l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_5 = hidden_states_4 = l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_5: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_5[0];  tag_activation_checkpoint_5 = None
        wrap_body_6 = self.wrap_body_6
        tag_activation_checkpoint_6 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_6, s77, hidden_states_5, l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_6 = hidden_states_5 = l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_6[0];  tag_activation_checkpoint_6 = None
        wrap_body_7 = self.wrap_body_7
        tag_activation_checkpoint_7 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_7, s77, hidden_states_6, l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_7 = hidden_states_6 = l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_7[0];  tag_activation_checkpoint_7 = None
        wrap_body_8 = self.wrap_body_8
        tag_activation_checkpoint_8 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_8, s77, hidden_states_7, l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_8 = hidden_states_7 = l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_8[0];  tag_activation_checkpoint_8 = None
        wrap_body_9 = self.wrap_body_9
        tag_activation_checkpoint_9 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_9, s77, hidden_states_8, l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_9 = hidden_states_8 = l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_9[0];  tag_activation_checkpoint_9 = None
        wrap_body_10 = self.wrap_body_10
        tag_activation_checkpoint_10 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_10, s77, hidden_states_9, l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_10 = hidden_states_9 = l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_10[0];  tag_activation_checkpoint_10 = None
        wrap_body_11 = self.wrap_body_11
        tag_activation_checkpoint_11 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_11, s77, hidden_states_10, l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_11 = hidden_states_10 = l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_11[0];  tag_activation_checkpoint_11 = None
        wrap_body_12 = self.wrap_body_12
        tag_activation_checkpoint_12 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_12, s77, hidden_states_11, l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_12 = hidden_states_11 = l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_12[0];  tag_activation_checkpoint_12 = None
        wrap_body_13 = self.wrap_body_13
        tag_activation_checkpoint_13 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_13, s77, hidden_states_12, l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_13 = hidden_states_12 = l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_13[0];  tag_activation_checkpoint_13 = None
        wrap_body_14 = self.wrap_body_14
        tag_activation_checkpoint_14 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_14, s77, hidden_states_13, l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_14 = hidden_states_13 = l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_14[0];  tag_activation_checkpoint_14 = None
        wrap_body_15 = self.wrap_body_15
        tag_activation_checkpoint_15 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_15, s77, hidden_states_14, l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_15 = hidden_states_14 = l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_15[0];  tag_activation_checkpoint_15 = None
        wrap_body_16 = self.wrap_body_16
        tag_activation_checkpoint_16 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_16, s77, hidden_states_15, l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_16 = hidden_states_15 = l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_16[0];  tag_activation_checkpoint_16 = None
        wrap_body_17 = self.wrap_body_17
        tag_activation_checkpoint_17 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_17, s77, hidden_states_16, l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_17 = hidden_states_16 = l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_17[0];  tag_activation_checkpoint_17 = None
        wrap_body_18 = self.wrap_body_18
        tag_activation_checkpoint_18 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_18, s77, hidden_states_17, l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_18 = hidden_states_17 = l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_18[0];  tag_activation_checkpoint_18 = None
        wrap_body_19 = self.wrap_body_19
        tag_activation_checkpoint_19 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_19, s77, hidden_states_18, l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_19 = hidden_states_18 = l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_19[0];  tag_activation_checkpoint_19 = None
        wrap_body_20 = self.wrap_body_20
        tag_activation_checkpoint_20 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_20, s77, hidden_states_19, l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_20 = hidden_states_19 = l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_20[0];  tag_activation_checkpoint_20 = None
        wrap_body_21 = self.wrap_body_21
        tag_activation_checkpoint_21 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_21, s77, hidden_states_20, l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_21 = hidden_states_20 = l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_21[0];  tag_activation_checkpoint_21 = None
        wrap_body_22 = self.wrap_body_22
        tag_activation_checkpoint_22 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_22, s77, hidden_states_21, l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_22 = hidden_states_21 = l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_22[0];  tag_activation_checkpoint_22 = None
        wrap_body_23 = self.wrap_body_23
        tag_activation_checkpoint_23 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_23, s77, hidden_states_22, l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_23 = hidden_states_22 = l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_23[0];  tag_activation_checkpoint_23 = None
        wrap_body_24 = self.wrap_body_24
        tag_activation_checkpoint_24 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_24, s77, hidden_states_23, l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_24 = hidden_states_23 = l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_24[0];  tag_activation_checkpoint_24 = None
        wrap_body_25 = self.wrap_body_25
        tag_activation_checkpoint_25 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_25, s77, hidden_states_24, l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_25 = hidden_states_24 = l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_25[0];  tag_activation_checkpoint_25 = None
        wrap_body_26 = self.wrap_body_26
        tag_activation_checkpoint_26 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_26, s77, hidden_states_25, l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_26 = hidden_states_25 = l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_26[0];  tag_activation_checkpoint_26 = None
        wrap_body_27 = self.wrap_body_27
        tag_activation_checkpoint_27 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_27, s77, hidden_states_26, l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_27 = hidden_states_26 = l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_27[0];  tag_activation_checkpoint_27 = None
        wrap_body_28 = self.wrap_body_28
        tag_activation_checkpoint_28 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_28, s77, hidden_states_27, l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_28 = hidden_states_27 = l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_28[0];  tag_activation_checkpoint_28 = None
        wrap_body_29 = self.wrap_body_29
        tag_activation_checkpoint_29 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_29, s77, hidden_states_28, l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_29 = hidden_states_28 = l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_29[0];  tag_activation_checkpoint_29 = None
        wrap_body_30 = self.wrap_body_30
        tag_activation_checkpoint_30 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_30, s77, hidden_states_29, l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_30 = hidden_states_29 = l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_30[0];  tag_activation_checkpoint_30 = None
        wrap_body_31 = self.wrap_body_31
        tag_activation_checkpoint_31 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_31, s77, hidden_states_30, l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_, cos_2, sin_2, causal_mask, l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, use_reentrant = True);  wrap_body_31 = s77 = hidden_states_30 = l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_ = cos_2 = sin_2 = causal_mask = l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = None
        hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = tag_activation_checkpoint_31[0];  tag_activation_checkpoint_31 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31.to(torch.float32);  hidden_states_31 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32.pow(2)
        variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_2: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
        rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_2);  add_2 = None
        hidden_states_33: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32 * rsqrt;  hidden_states_32 = rsqrt = None

         # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_33.to(torch.float32);  hidden_states_33 = None
        hidden_states_34: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_norm_parameters_weight_ * to_7;  l_self_modules_norm_parameters_weight_ = to_7 = None
        return (hidden_states_34,)

    class wrap_body_0(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", l_kwargs_inputs_embeds_: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_kwargs_inputs_embeds_.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_1.to(torch.float32);  hidden_states_1 = None
            hidden_states_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_2.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_3: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_3.reshape(6, 32, getitem_25, 128);  hidden_states_3 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_4: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_4.reshape(6, 32, getitem_30, 128);  hidden_states_4 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_5: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_kwargs_inputs_embeds_ + attn_output_3;  l_kwargs_inputs_embeds_ = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_5.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6 * rsqrt_1;  hidden_states_6 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7.to(torch.float32);  hidden_states_7 = None
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_8, l_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_8, l_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_8 = l_self_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_5 + down_proj;  hidden_states_5 = down_proj = None
            return (hidden_states_9,)

    class wrap_body_1(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_1.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_1 * rsqrt;  hidden_states_1 = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_2.to(torch.float32);  hidden_states_2 = None
            hidden_states_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_3.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_3, l_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_1_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_3, l_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_1_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_3, l_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_3 = l_self_modules_layers_modules_1_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_4: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_4.reshape(6, 32, getitem_25, 128);  hidden_states_4 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_5: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_5.reshape(6, 32, getitem_30, 128);  hidden_states_5 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states + attn_output_3;  hidden_states = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7 * rsqrt_1;  hidden_states_7 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8.to(torch.float32);  hidden_states_8 = None
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_1_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_9 = l_self_modules_layers_modules_1_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6 + down_proj;  hidden_states_6 = down_proj = None
            return (hidden_states_10,)

    class wrap_body_2(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_1: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_1.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_2.to(torch.float32);  hidden_states_2 = None
            hidden_states_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_3.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_3, l_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_2_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_3, l_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_2_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_3, l_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_3 = l_self_modules_layers_modules_2_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_4: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_4.reshape(6, 32, getitem_25, 128);  hidden_states_4 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_5: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_5.reshape(6, 32, getitem_30, 128);  hidden_states_5 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_1 + attn_output_3;  hidden_states_1 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7 * rsqrt_1;  hidden_states_7 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8.to(torch.float32);  hidden_states_8 = None
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_2_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_9 = l_self_modules_layers_modules_2_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6 + down_proj;  hidden_states_6 = down_proj = None
            return (hidden_states_10,)

    class wrap_body_3(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_2: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_2.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_3.to(torch.float32);  hidden_states_3 = None
            hidden_states_4: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_4.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_4, l_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_3_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_4, l_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_3_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_4, l_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_4 = l_self_modules_layers_modules_3_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_5: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_5.reshape(6, 32, getitem_25, 128);  hidden_states_5 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_6: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_6.reshape(6, 32, getitem_30, 128);  hidden_states_6 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_2 + attn_output_3;  hidden_states_2 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8 * rsqrt_1;  hidden_states_8 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9.to(torch.float32);  hidden_states_9 = None
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_10, l_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_3_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_10, l_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_10 = l_self_modules_layers_modules_3_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7 + down_proj;  hidden_states_7 = down_proj = None
            return (hidden_states_11,)

    class wrap_body_4(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_3: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_3.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_4: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_4.to(torch.float32);  hidden_states_4 = None
            hidden_states_5: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_5.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_5, l_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_4_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_5, l_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_4_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_5, l_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_5 = l_self_modules_layers_modules_4_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_6: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_6.reshape(6, 32, getitem_25, 128);  hidden_states_6 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_7: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_7.reshape(6, 32, getitem_30, 128);  hidden_states_7 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_3 + attn_output_3;  hidden_states_3 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9 * rsqrt_1;  hidden_states_9 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10.to(torch.float32);  hidden_states_10 = None
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_11, l_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_4_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_11, l_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_11 = l_self_modules_layers_modules_4_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8 + down_proj;  hidden_states_8 = down_proj = None
            return (hidden_states_12,)

    class wrap_body_5(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_4: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_4.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_5: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_5.to(torch.float32);  hidden_states_5 = None
            hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_6.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_6, l_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_5_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_6, l_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_5_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_6, l_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_6 = l_self_modules_layers_modules_5_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_7: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_7.reshape(6, 32, getitem_25, 128);  hidden_states_7 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_8: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_8.reshape(6, 32, getitem_30, 128);  hidden_states_8 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_4 + attn_output_3;  hidden_states_4 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10 * rsqrt_1;  hidden_states_10 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11.to(torch.float32);  hidden_states_11 = None
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_12, l_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_5_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_12, l_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_12 = l_self_modules_layers_modules_5_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9 + down_proj;  hidden_states_9 = down_proj = None
            return (hidden_states_13,)

    class wrap_body_6(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_5: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_5.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6.to(torch.float32);  hidden_states_6 = None
            hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_7.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_7, l_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_6_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_7, l_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_6_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_7, l_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_7 = l_self_modules_layers_modules_6_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_8: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_8.reshape(6, 32, getitem_25, 128);  hidden_states_8 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_9: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_9.reshape(6, 32, getitem_30, 128);  hidden_states_9 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_5 + attn_output_3;  hidden_states_5 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11 * rsqrt_1;  hidden_states_11 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12.to(torch.float32);  hidden_states_12 = None
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_6_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_13 = l_self_modules_layers_modules_6_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10 + down_proj;  hidden_states_10 = down_proj = None
            return (hidden_states_14,)

    class wrap_body_7(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_6: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7.to(torch.float32);  hidden_states_7 = None
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_8.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_8, l_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_7_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_8, l_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_7_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_8, l_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_8 = l_self_modules_layers_modules_7_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_9: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_9.reshape(6, 32, getitem_25, 128);  hidden_states_9 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_10: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_10.reshape(6, 32, getitem_30, 128);  hidden_states_10 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_6 + attn_output_3;  hidden_states_6 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12 * rsqrt_1;  hidden_states_12 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13.to(torch.float32);  hidden_states_13 = None
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_14, l_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_7_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_14, l_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_14 = l_self_modules_layers_modules_7_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11 + down_proj;  hidden_states_11 = down_proj = None
            return (hidden_states_15,)

    class wrap_body_8(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_7: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8.to(torch.float32);  hidden_states_8 = None
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_9.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_8_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_8_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_9, l_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_9 = l_self_modules_layers_modules_8_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_10: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_10.reshape(6, 32, getitem_25, 128);  hidden_states_10 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_11: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_11.reshape(6, 32, getitem_30, 128);  hidden_states_11 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_7 + attn_output_3;  hidden_states_7 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13 * rsqrt_1;  hidden_states_13 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14.to(torch.float32);  hidden_states_14 = None
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_15, l_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_8_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_15, l_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_15 = l_self_modules_layers_modules_8_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12 + down_proj;  hidden_states_12 = down_proj = None
            return (hidden_states_16,)

    class wrap_body_9(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_8: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9.to(torch.float32);  hidden_states_9 = None
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_10.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_10, l_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_9_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_10, l_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_9_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_10, l_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_10 = l_self_modules_layers_modules_9_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_11: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_11.reshape(6, 32, getitem_25, 128);  hidden_states_11 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_12: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_12.reshape(6, 32, getitem_30, 128);  hidden_states_12 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_8 + attn_output_3;  hidden_states_8 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14 * rsqrt_1;  hidden_states_14 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15.to(torch.float32);  hidden_states_15 = None
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_16, l_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_9_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_16, l_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_16 = l_self_modules_layers_modules_9_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13 + down_proj;  hidden_states_13 = down_proj = None
            return (hidden_states_17,)

    class wrap_body_10(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_9: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10.to(torch.float32);  hidden_states_10 = None
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_11.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_11, l_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_10_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_11, l_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_10_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_11, l_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_11 = l_self_modules_layers_modules_10_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_12: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_12.reshape(6, 32, getitem_25, 128);  hidden_states_12 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_13: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_13.reshape(6, 32, getitem_30, 128);  hidden_states_13 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_9 + attn_output_3;  hidden_states_9 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15 * rsqrt_1;  hidden_states_15 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16.to(torch.float32);  hidden_states_16 = None
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_17, l_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_10_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_17, l_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_17 = l_self_modules_layers_modules_10_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14 + down_proj;  hidden_states_14 = down_proj = None
            return (hidden_states_18,)

    class wrap_body_11(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_10: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11.to(torch.float32);  hidden_states_11 = None
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_12.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_12, l_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_11_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_12, l_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_11_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_12, l_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_12 = l_self_modules_layers_modules_11_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_13: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_13.reshape(6, 32, getitem_25, 128);  hidden_states_13 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_14: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_14.reshape(6, 32, getitem_30, 128);  hidden_states_14 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_10 + attn_output_3;  hidden_states_10 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16 * rsqrt_1;  hidden_states_16 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17.to(torch.float32);  hidden_states_17 = None
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_18, l_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_11_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_18, l_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_18 = l_self_modules_layers_modules_11_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15 + down_proj;  hidden_states_15 = down_proj = None
            return (hidden_states_19,)

    class wrap_body_12(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_11: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12.to(torch.float32);  hidden_states_12 = None
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_13.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_12_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_12_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_13, l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_13 = l_self_modules_layers_modules_12_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_14: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_14.reshape(6, 32, getitem_25, 128);  hidden_states_14 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_15: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_15.reshape(6, 32, getitem_30, 128);  hidden_states_15 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_11 + attn_output_3;  hidden_states_11 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17 * rsqrt_1;  hidden_states_17 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18.to(torch.float32);  hidden_states_18 = None
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_12_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_19 = l_self_modules_layers_modules_12_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16 + down_proj;  hidden_states_16 = down_proj = None
            return (hidden_states_20,)

    class wrap_body_13(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_12: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13.to(torch.float32);  hidden_states_13 = None
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_14.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_14, l_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_13_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_14, l_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_13_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_14, l_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_14 = l_self_modules_layers_modules_13_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_15: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_15.reshape(6, 32, getitem_25, 128);  hidden_states_15 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_16: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_16.reshape(6, 32, getitem_30, 128);  hidden_states_16 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_12 + attn_output_3;  hidden_states_12 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18 * rsqrt_1;  hidden_states_18 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19.to(torch.float32);  hidden_states_19 = None
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_20, l_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_13_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_20, l_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_20 = l_self_modules_layers_modules_13_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17 + down_proj;  hidden_states_17 = down_proj = None
            return (hidden_states_21,)

    class wrap_body_14(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_13: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14.to(torch.float32);  hidden_states_14 = None
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_15.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_15, l_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_14_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_15, l_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_14_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_15, l_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_15 = l_self_modules_layers_modules_14_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_16: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_16.reshape(6, 32, getitem_25, 128);  hidden_states_16 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_17: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_17.reshape(6, 32, getitem_30, 128);  hidden_states_17 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_13 + attn_output_3;  hidden_states_13 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19 * rsqrt_1;  hidden_states_19 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20.to(torch.float32);  hidden_states_20 = None
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_21, l_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_14_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_21, l_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_21 = l_self_modules_layers_modules_14_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18 + down_proj;  hidden_states_18 = down_proj = None
            return (hidden_states_22,)

    class wrap_body_15(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_14: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15.to(torch.float32);  hidden_states_15 = None
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_16.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_16, l_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_15_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_16, l_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_15_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_16, l_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_16 = l_self_modules_layers_modules_15_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_17: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_17.reshape(6, 32, getitem_25, 128);  hidden_states_17 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_18: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_18.reshape(6, 32, getitem_30, 128);  hidden_states_18 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_14 + attn_output_3;  hidden_states_14 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20 * rsqrt_1;  hidden_states_20 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21.to(torch.float32);  hidden_states_21 = None
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_22, l_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_15_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_22, l_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_22 = l_self_modules_layers_modules_15_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19 + down_proj;  hidden_states_19 = down_proj = None
            return (hidden_states_23,)

    class wrap_body_16(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_15: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16.to(torch.float32);  hidden_states_16 = None
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_17.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_17, l_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_16_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_17, l_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_16_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_17, l_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_17 = l_self_modules_layers_modules_16_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_18: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_18.reshape(6, 32, getitem_25, 128);  hidden_states_18 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_19: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_19.reshape(6, 32, getitem_30, 128);  hidden_states_19 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_15 + attn_output_3;  hidden_states_15 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21 * rsqrt_1;  hidden_states_21 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22.to(torch.float32);  hidden_states_22 = None
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_23, l_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_16_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_23, l_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_23 = l_self_modules_layers_modules_16_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20 + down_proj;  hidden_states_20 = down_proj = None
            return (hidden_states_24,)

    class wrap_body_17(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_16: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17.to(torch.float32);  hidden_states_17 = None
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_18.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_18, l_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_17_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_18, l_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_17_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_18, l_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_18 = l_self_modules_layers_modules_17_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_19: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_19.reshape(6, 32, getitem_25, 128);  hidden_states_19 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_20: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_20.reshape(6, 32, getitem_30, 128);  hidden_states_20 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_16 + attn_output_3;  hidden_states_16 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22 * rsqrt_1;  hidden_states_22 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23.to(torch.float32);  hidden_states_23 = None
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_24, l_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_17_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_24, l_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_24 = l_self_modules_layers_modules_17_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21 + down_proj;  hidden_states_21 = down_proj = None
            return (hidden_states_25,)

    class wrap_body_18(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_17: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18.to(torch.float32);  hidden_states_18 = None
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_19.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_18_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_18_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_19, l_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_19 = l_self_modules_layers_modules_18_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_20: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_20.reshape(6, 32, getitem_25, 128);  hidden_states_20 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_21: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_21.reshape(6, 32, getitem_30, 128);  hidden_states_21 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_17 + attn_output_3;  hidden_states_17 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23 * rsqrt_1;  hidden_states_23 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24.to(torch.float32);  hidden_states_24 = None
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_25, l_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_18_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_25, l_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_25 = l_self_modules_layers_modules_18_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22 + down_proj;  hidden_states_22 = down_proj = None
            return (hidden_states_26,)

    class wrap_body_19(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_18: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19.to(torch.float32);  hidden_states_19 = None
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_20.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_20, l_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_19_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_20, l_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_19_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_20, l_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_20 = l_self_modules_layers_modules_19_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_21: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_21.reshape(6, 32, getitem_25, 128);  hidden_states_21 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_22: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_22.reshape(6, 32, getitem_30, 128);  hidden_states_22 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_18 + attn_output_3;  hidden_states_18 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24 * rsqrt_1;  hidden_states_24 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25.to(torch.float32);  hidden_states_25 = None
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_26, l_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_19_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_26, l_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_26 = l_self_modules_layers_modules_19_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23 + down_proj;  hidden_states_23 = down_proj = None
            return (hidden_states_27,)

    class wrap_body_20(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_19: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20.to(torch.float32);  hidden_states_20 = None
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_21.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_21, l_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_20_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_21, l_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_20_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_21, l_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_21 = l_self_modules_layers_modules_20_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_22: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_22.reshape(6, 32, getitem_25, 128);  hidden_states_22 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_23: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_23.reshape(6, 32, getitem_30, 128);  hidden_states_23 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_19 + attn_output_3;  hidden_states_19 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25 * rsqrt_1;  hidden_states_25 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26.to(torch.float32);  hidden_states_26 = None
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_27, l_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_20_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_27, l_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_27 = l_self_modules_layers_modules_20_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24 + down_proj;  hidden_states_24 = down_proj = None
            return (hidden_states_28,)

    class wrap_body_21(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_20: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21.to(torch.float32);  hidden_states_21 = None
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_22.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_22, l_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_21_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_22, l_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_21_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_22, l_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_22 = l_self_modules_layers_modules_21_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_23: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_23.reshape(6, 32, getitem_25, 128);  hidden_states_23 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_24: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_24.reshape(6, 32, getitem_30, 128);  hidden_states_24 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_20 + attn_output_3;  hidden_states_20 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26 * rsqrt_1;  hidden_states_26 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27.to(torch.float32);  hidden_states_27 = None
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_28, l_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_21_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_28, l_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_28 = l_self_modules_layers_modules_21_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25 + down_proj;  hidden_states_25 = down_proj = None
            return (hidden_states_29,)

    class wrap_body_22(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_21: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22.to(torch.float32);  hidden_states_22 = None
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_23.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_23, l_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_22_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_23, l_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_22_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_23, l_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_23 = l_self_modules_layers_modules_22_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_24: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_24.reshape(6, 32, getitem_25, 128);  hidden_states_24 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_25: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_25.reshape(6, 32, getitem_30, 128);  hidden_states_25 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_21 + attn_output_3;  hidden_states_21 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27 * rsqrt_1;  hidden_states_27 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28.to(torch.float32);  hidden_states_28 = None
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_29, l_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_22_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_29, l_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_29 = l_self_modules_layers_modules_22_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26 + down_proj;  hidden_states_26 = down_proj = None
            return (hidden_states_30,)

    class wrap_body_23(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_22: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23.to(torch.float32);  hidden_states_23 = None
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_24.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_24, l_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_23_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_24, l_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_23_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_24, l_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_24 = l_self_modules_layers_modules_23_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_25: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_25.reshape(6, 32, getitem_25, 128);  hidden_states_25 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_26: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_26.reshape(6, 32, getitem_30, 128);  hidden_states_26 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_22 + attn_output_3;  hidden_states_22 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28 * rsqrt_1;  hidden_states_28 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29.to(torch.float32);  hidden_states_29 = None
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_30, l_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_23_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_30, l_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_30 = l_self_modules_layers_modules_23_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27 + down_proj;  hidden_states_27 = down_proj = None
            return (hidden_states_31,)

    class wrap_body_24(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_23: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24.to(torch.float32);  hidden_states_24 = None
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_25.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_25, l_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_24_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_25, l_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_24_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_25, l_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_25 = l_self_modules_layers_modules_24_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_26: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_26.reshape(6, 32, getitem_25, 128);  hidden_states_26 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_27: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_27.reshape(6, 32, getitem_30, 128);  hidden_states_27 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_23 + attn_output_3;  hidden_states_23 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29 * rsqrt_1;  hidden_states_29 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30.to(torch.float32);  hidden_states_30 = None
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_31, l_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_24_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_31, l_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_31 = l_self_modules_layers_modules_24_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28 + down_proj;  hidden_states_28 = down_proj = None
            return (hidden_states_32,)

    class wrap_body_25(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_24: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25.to(torch.float32);  hidden_states_25 = None
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_26.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_26, l_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_25_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_26, l_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_25_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_26, l_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_26 = l_self_modules_layers_modules_25_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_27: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_27.reshape(6, 32, getitem_25, 128);  hidden_states_27 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_28: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_28.reshape(6, 32, getitem_30, 128);  hidden_states_28 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_24 + attn_output_3;  hidden_states_24 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30 * rsqrt_1;  hidden_states_30 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31.to(torch.float32);  hidden_states_31 = None
            hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_32, l_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_25_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_32, l_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_32 = l_self_modules_layers_modules_25_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_33: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29 + down_proj;  hidden_states_29 = down_proj = None
            return (hidden_states_33,)

    class wrap_body_26(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_25: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26.to(torch.float32);  hidden_states_26 = None
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_27.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_27, l_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_26_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_27, l_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_26_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_27, l_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_27 = l_self_modules_layers_modules_26_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_28: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_28.reshape(6, 32, getitem_25, 128);  hidden_states_28 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_29: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_29.reshape(6, 32, getitem_30, 128);  hidden_states_29 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_25 + attn_output_3;  hidden_states_25 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31 * rsqrt_1;  hidden_states_31 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32.to(torch.float32);  hidden_states_32 = None
            hidden_states_33: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_33, l_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_26_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_33, l_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_33 = l_self_modules_layers_modules_26_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_34: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30 + down_proj;  hidden_states_30 = down_proj = None
            return (hidden_states_34,)

    class wrap_body_27(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_26: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27.to(torch.float32);  hidden_states_27 = None
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_28.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_28, l_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_27_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_28, l_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_27_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_28, l_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_28 = l_self_modules_layers_modules_27_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_29: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_29.reshape(6, 32, getitem_25, 128);  hidden_states_29 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_30: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_30.reshape(6, 32, getitem_30, 128);  hidden_states_30 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_26 + attn_output_3;  hidden_states_26 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_33: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32 * rsqrt_1;  hidden_states_32 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_33.to(torch.float32);  hidden_states_33 = None
            hidden_states_34: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_34, l_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_27_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_34, l_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_34 = l_self_modules_layers_modules_27_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_35: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31 + down_proj;  hidden_states_31 = down_proj = None
            return (hidden_states_35,)

    class wrap_body_28(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_27: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28.to(torch.float32);  hidden_states_28 = None
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_29.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_29, l_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_28_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_29, l_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_28_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_29, l_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_29 = l_self_modules_layers_modules_28_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_30: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_30.reshape(6, 32, getitem_25, 128);  hidden_states_30 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_31: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_31.reshape(6, 32, getitem_30, 128);  hidden_states_31 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_27 + attn_output_3;  hidden_states_27 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_33: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_33.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_34: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_33 * rsqrt_1;  hidden_states_33 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_34.to(torch.float32);  hidden_states_34 = None
            hidden_states_35: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_35, l_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_28_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_35, l_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_35 = l_self_modules_layers_modules_28_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_36: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_32 + down_proj;  hidden_states_32 = down_proj = None
            return (hidden_states_36,)

    class wrap_body_29(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_28: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29.to(torch.float32);  hidden_states_29 = None
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_30.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_30, l_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_29_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_30, l_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_29_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_30, l_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_30 = l_self_modules_layers_modules_29_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_31: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_31.reshape(6, 32, getitem_25, 128);  hidden_states_31 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_32: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_32.reshape(6, 32, getitem_30, 128);  hidden_states_32 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_33: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_28 + attn_output_3;  hidden_states_28 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_34: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_33.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_34.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_35: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_34 * rsqrt_1;  hidden_states_34 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_35.to(torch.float32);  hidden_states_35 = None
            hidden_states_36: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_36, l_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_29_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_36, l_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_36 = l_self_modules_layers_modules_29_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_37: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_33 + down_proj;  hidden_states_33 = down_proj = None
            return (hidden_states_37,)

    class wrap_body_30(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_29: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30.to(torch.float32);  hidden_states_30 = None
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_31.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_31, l_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_30_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_31, l_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_30_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_31, l_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_31 = l_self_modules_layers_modules_30_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_32: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_32.reshape(6, 32, getitem_25, 128);  hidden_states_32 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_33: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_33.reshape(6, 32, getitem_30, 128);  hidden_states_33 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_34: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_29 + attn_output_3;  hidden_states_29 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_35: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_34.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_35.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_36: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_35 * rsqrt_1;  hidden_states_35 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_36.to(torch.float32);  hidden_states_36 = None
            hidden_states_37: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_37, l_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_30_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_37, l_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_37 = l_self_modules_layers_modules_30_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_38: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_34 + down_proj;  hidden_states_34 = down_proj = None
            return (hidden_states_38,)

    class wrap_body_31(torch.nn.Module):
        def forward(self, s77: "Sym(s77)", hidden_states_30: "f32[6, s77, 4096][4096*s77, 4096, 1]", l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", l_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_: "f16[1024, 4096][4096, 1]", cos_2: "f32[1, s77, 128][128*s77, 128, 1]", sin_2: "f32[1, s77, 128][128*s77, 128, 1]", causal_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]", l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_: "f16[4096, 4096][4096, 1]", l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_: "f16[4096][1]", l_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_: "f16[14336, 4096][4096, 1]", l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_: "f16[4096, 14336][14336, 1]"):
             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states.pow(2)
            variance: "f32[6, s77, 1][s77, 1, 1]" = pow_1.mean(-1, keepdim = True);  pow_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[6, s77, 1][s77, 1, 1]" = variance + 1e-05;  variance = None
            rsqrt: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add);  add = None
            hidden_states_31: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states * rsqrt;  hidden_states = rsqrt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_31.to(torch.float32);  hidden_states_31 = None
            hidden_states_32: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = to_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:148 in forward, code: input_shape = hidden_states.shape[:-1]
            size = hidden_states_32.size()
            getitem = size[0];  getitem = None
            getitem_1: "Sym(s77)" = size[1]
            getitem_2 = size[2];  size = getitem_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:151 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(hidden_states_32, l_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_31_modules_self_attn_modules_q_proj_parameters_weight_ = None
            view: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = linear.view((6, getitem_1, -1, 128));  linear = None
            query_states: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = view.transpose(1, 2);  view = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:152 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_1: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_32, l_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_31_modules_self_attn_modules_k_proj_parameters_weight_ = None
            view_1: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_1.view((6, getitem_1, -1, 128));  linear_1 = None
            key_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_1.transpose(1, 2);  view_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:153 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
            linear_2: "bf16[6, s77, 1024][1024*s77, 1024, 1]" = torch._C._nn.linear(hidden_states_32, l_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_32 = l_self_modules_layers_modules_31_modules_self_attn_modules_v_proj_parameters_weight_ = None
            view_2: "bf16[6, s77, 8, 128][1024*s77, 1024, 128, 1]" = linear_2.view((6, getitem_1, -1, 128));  linear_2 = None
            value_states: "bf16[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = view_2.transpose(1, 2);  view_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:77 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
            cos: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = cos_2.unsqueeze(1);  cos_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:78 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
            sin: "f32[1, 1, s77, 128][128*s77, 128*s77, 128, 1]" = sin_2.unsqueeze(1);  sin_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = query_states * cos

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_1 = query_states.size()
            getitem_3 = size_1[0];  getitem_3 = None
            getitem_4 = size_1[1];  getitem_4 = None
            getitem_5: "Sym(s77)" = size_1[2];  getitem_5 = None
            getitem_6 = size_1[3];  size_1 = getitem_6 = None
            x1: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_2 = query_states.size()
            getitem_8 = size_2[0];  getitem_8 = None
            getitem_9 = size_2[1];  getitem_9 = None
            getitem_10: "Sym(s77)" = size_2[2];  getitem_10 = None
            getitem_11 = size_2[3];  size_2 = getitem_11 = None
            x2: "bf16[6, 32, s77, 64][4096*s77, 128, 4096, 1]" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[6, 32, s77, 64][2048*s77, 64, 2048, 1]" = -x2;  x2 = None
            cat: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = torch.cat((neg, x1), dim = -1);  neg = x1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:79 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = cat * sin;  cat = None
            q_embed: "f32[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = key_states * cos;  cos = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:52 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            size_3 = key_states.size()
            getitem_13 = size_3[0];  getitem_13 = None
            getitem_14 = size_3[1];  getitem_14 = None
            getitem_15: "Sym(s77)" = size_3[2];  getitem_15 = None
            getitem_16 = size_3[3];  size_3 = getitem_16 = None
            x1_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(None, 64, None))]

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:53 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            size_4 = key_states.size()
            getitem_18 = size_4[0];  getitem_18 = None
            getitem_19 = size_4[1];  getitem_19 = None
            getitem_20: "Sym(s77)" = size_4[2];  getitem_20 = None
            getitem_21 = size_4[3];  size_4 = getitem_21 = None
            x2_1: "bf16[6, 8, s77, 64][1024*s77, 128, 1024, 1]" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:54 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[6, 8, s77, 64][512*s77, 64, 512, 1]" = -x2_1;  x2_1 = None
            cat_1: "bf16[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:80 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 8, s77, 128][1024*s77, 128*s77, 128, 1]" = cat_1 * sin;  cat_1 = sin = None
            k_embed: "f32[6, 8, s77, 128][1024*s77, 128, 1024, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_5 = k_embed.size()
            getitem_23 = size_5[0];  getitem_23 = None
            getitem_24 = size_5[1];  getitem_24 = None
            getitem_25: "Sym(s77)" = size_5[2]
            getitem_26 = size_5[3];  size_5 = getitem_26 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_27: "f32[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = k_embed[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  k_embed = None
            hidden_states_33: "f32[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_27.expand(6, 8, 4, getitem_25, 128);  getitem_27 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            key: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_33.reshape(6, 32, getitem_25, 128);  hidden_states_33 = getitem_25 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:22 in repeat_kv, code: batch, num_key_value_heads, slen, head_dim = hidden_states.shape
            size_6 = value_states.size()
            getitem_28 = size_6[0];  getitem_28 = None
            getitem_29 = size_6[1];  getitem_29 = None
            getitem_30: "Sym(s77)" = size_6[2]
            getitem_31 = size_6[3];  size_6 = getitem_31 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:25 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            getitem_32: "bf16[6, 8, 1, s77, 128][1024*s77, 128, 1024*s77, 1024, 1]" = value_states[(slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  value_states = None
            hidden_states_34: "bf16[6, 8, 4, s77, 128][1024*s77, 128, 0, 1024, 1]" = getitem_32.expand(6, 8, 4, getitem_30, 128);  getitem_32 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:26 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            value: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = hidden_states_34.reshape(6, 32, getitem_30, 128);  hidden_states_34 = getitem_30 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:68 in sdpa_attention_forward, code: attention_mask = attention_mask[:, :, :, : key.shape[-2]]
            size_7 = key.size()
            getitem_33 = size_7[0];  getitem_33 = None
            getitem_34 = size_7[1];  getitem_34 = None
            getitem_35: "Sym(s77)" = size_7[2]
            getitem_36 = size_7[3];  size_7 = getitem_36 = None
            attention_mask: "b8[6, 1, s77, s77][s77**2, s77**2, s77, 1]" = causal_mask[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, getitem_35, None))];  causal_mask = getitem_35 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:72 in sdpa_attention_forward, code: query = query.contiguous()
            query: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = q_embed.contiguous();  q_embed = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:73 in sdpa_attention_forward, code: key = key.contiguous()
            key_1: "f32[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = key.contiguous();  key = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:74 in sdpa_attention_forward, code: value = value.contiguous()
            value_1: "bf16[6, 32, s77, 128][4096*s77, 128*s77, 128, 1]" = value.contiguous();  value = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:82 in sdpa_attention_forward, code: is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, "is_causal", True)
            size_8 = query.size()
            getitem_38 = size_8[0];  getitem_38 = None
            getitem_39 = size_8[1];  getitem_39 = None
            getitem_40: "Sym(s77)" = size_8[2]
            getitem_41 = size_8[3];  size_8 = getitem_41 = None
            gt: "Sym(s77 > 1)" = getitem_40 > 1;  getitem_40 = gt = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:89 in sdpa_attention_forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(
            attn_output: "bf16[6, 32, s77, 128][4096*s77, 128, 4096, 1]" = torch._C._nn.scaled_dot_product_attention(query, key_1, value_1, attn_mask = attention_mask, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  query = key_1 = value_1 = attention_mask = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:99 in sdpa_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_3: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = attn_output.transpose(1, 2);  attn_output = None
            attn_output_1: "bf16[6, s77, 32, 128][4096*s77, 4096, 128, 1]" = transpose_3.contiguous();  transpose_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:179 in forward, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
            reshape_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = attn_output_1.reshape(6, getitem_1, -1);  attn_output_1 = getitem_1 = None
            attn_output_2: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = reshape_2.contiguous();  reshape_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:180 in forward, code: attn_output = self.o_proj(attn_output)
            attn_output_3: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(attn_output_2, l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:238 in forward, code: hidden_states = residual + hidden_states
            hidden_states_35: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_30 + attn_output_3;  hidden_states_30 = attn_output_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:196 in forward, code: hidden_states = hidden_states.to(torch.float32)
            hidden_states_36: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_35.to(torch.float32)

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:197 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_36.pow(2)
            variance_1: "f32[6, s77, 1][s77, 1, 1]" = pow_2.mean(-1, keepdim = True);  pow_2 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:198 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[6, s77, 1][s77, 1, 1]" = variance_1 + 1e-05;  variance_1 = None
            rsqrt_1: "f32[6, s77, 1][s77, 1, 1]" = torch.rsqrt(add_4);  add_4 = None
            hidden_states_37: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_36 * rsqrt_1;  hidden_states_36 = rsqrt_1 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:199 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_3: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_37.to(torch.float32);  hidden_states_37 = None
            hidden_states_38: "f32[6, s77, 4096][4096*s77, 4096, 1]" = l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ * to_3;  l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = to_3 = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:46 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
            linear_4: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_38, l_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_self_modules_layers_modules_31_modules_mlp_modules_gate_proj_parameters_weight_ = None
            silu: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
            linear_5: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = torch._C._nn.linear(hidden_states_38, l_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_38 = l_self_modules_layers_modules_31_modules_mlp_modules_up_proj_parameters_weight_ = None
            mul_8: "bf16[6, s77, 14336][14336*s77, 14336, 1]" = silu * linear_5;  silu = linear_5 = None
            down_proj: "bf16[6, s77, 4096][4096*s77, 4096, 1]" = torch._C._nn.linear(mul_8, l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_8 = l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = None

             # File: /home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:244 in forward, code: hidden_states = residual + hidden_states
            hidden_states_39: "f32[6, s77, 4096][4096*s77, 4096, 1]" = hidden_states_35 + down_proj;  hidden_states_35 = down_proj = None
            return (hidden_states_39,)


Original traceback:
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/utils/generic.py", line 1083, in torch_dynamo_resume_in_wrapper_at_1005
    outputs = func(self, *args, **kwargs)
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py", line 364, in forward
    hidden_states = decoder_layer(
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/modeling_layers.py", line 92, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)


Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
