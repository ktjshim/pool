= = = = = = = = = = = = = = = = = = = =
## Starting Time: 09-11 11:51:37
Namespace(dataset='huggingface', llm='Mistral-7B', llm_model_path='', seed=0, device='cuda:0', max_txt_length=512, max_ans_length=512, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_output_dim=4096, n_layers=2, gnn_type='SAGE', max_degree=300, num_epochs=2, batch_size=6, eval_batch_size=6, patience=2, lr=1e-05, wd=0.05, output_dir='output', grad_steps=4, name='centrality-0911')

[Training Data] # Chain Samples 1536 (51.20)
[Data Split] # Train 3000  # Test 500
# Train 2400   # Val 600   # Test 500
Loading checkpoint shards: 100%|█████████████████████████████████| 3/3 [00:01<00:00,  2.12it/s]
Finish loading pre-trained Mistral-7B model!
Trainable params 12950528 || all params 7254682624 || trainable% 0.17851
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(row_sum, -0.5).flatten()
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:19: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:653.)
  return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))
  0%|                                                                  | 0/800 [00:00<?, ?it/s]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|                                                        | 1/800 [00:36<8:01:02, 36.12s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  0%|▏                                                      | 2/800 [01:32<10:39:50, 48.11s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  0%|▏                                                       | 3/800 [01:33<5:50:55, 26.42s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  0%|▎                                                       | 4/800 [01:33<3:34:42, 16.18s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  1%|▎                                                       | 5/800 [01:34<2:19:40, 10.54s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  1%|▍                                                       | 6/800 [01:34<1:34:29,  7.14s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  1%|▍                                                       | 7/800 [01:35<1:05:42,  4.97s/it]/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
  1%|▌                                                         | 8/800 [01:35<47:10,  3.57s/it]W0911 12:53:17.921000 3579909 torch/_dynamo/convert_frame.py:1016] [0/8] torch._dynamo hit config.recompile_limit (8)
W0911 12:53:17.921000 3579909 torch/_dynamo/convert_frame.py:1016] [0/8]    function: 'forward' (/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:85)
W0911 12:53:17.921000 3579909 torch/_dynamo/convert_frame.py:1016] [0/8]    last reason: 0/7: samples['request'][0] == '# TASK LIST #:\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\n\n# GOAL #\nPlease understand the user\'s request and generate task steps and task invocation graph to solve it.\n\n# REQUIREMENT #\n1. The format must in a strict JSON format as {"task_steps": [ concrete step descriptions ], "task_nodes": [ a list of tasks to be executed in sequence to fulfill user\'s request ], "task_links": [{"source": "task name i", "target": "task name j"}]}\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\n\n\n# USER REQUEST #: I have a noisy audio file \'example.wav\' containing a lecture. I want to get a summarized text of it, then ask a question related to the lecture content based on an image \'example.jpg\', and finally, I want an answer to my question \'What is the main topic of the lecture?\'\nNow please generate your result in a strict JSON format:\n# RESULT #:'
W0911 12:53:17.921000 3579909 torch/_dynamo/convert_frame.py:1016] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W0911 12:53:17.921000 3579909 torch/_dynamo/convert_frame.py:1016] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/nas/home/ktshim/tool/pool/graphtoken/glm_node_centrality.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
 25%|██████████████                                          | 200/800 [03:09<04:57,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

--- Running Inference Check at Step 200 ---
 50%|████████████████████████████                            | 400/800 [04:58<03:32,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

--- Running Inference Check at Step 400 ---
Epoch 0|2: Train Loss (Epoch Mean): 0.5951713850721717
Epoch: 0|2: Val Loss: 0.3918169939517975
Saving checkpoint at epoch 0 to output/huggingface/Mistral-7B/SAGE_Epoch2_checkpoint_best.pth
 75%|██████████████████████████████████████████              | 600/800 [07:14<01:36,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

--- Running Inference Check at Step 200 ---
100%|████████████████████████████████████████████████████████| 800/800 [08:59<00:00,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

--- Running Inference Check at Step 400 ---
Epoch 1|2: Train Loss (Epoch Mean): 0.3558675394952297
Epoch: 1|2: Val Loss: 0.36415170654654505
Saving checkpoint at epoch 1 to output/huggingface/Mistral-7B/SAGE_Epoch2_checkpoint_best.pth
/home/ktshim/.local/lib/python3.12/site-packages/torch/cuda/memory.py:491: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Loading checkpoint from output/huggingface/Mistral-7B/SAGE_Epoch2_checkpoint_best.pth
                                           Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  0%|               | 0/84 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  1%|       | 1/84 [00:10<14:26, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  2%|▏      | 2/84 [00:21<14:52, 10.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  4%|▎      | 3/84 [00:38<18:24, 13.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  5%|▎      | 4/84 [00:48<16:12, 12.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  6%|▍      | 5/84 [00:58<14:54, 11.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  7%|▌      | 6/84 [01:13<16:18, 12.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  8%|▌      | 7/84 [01:23<15:06, 11.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 10%|▋      | 8/84 [01:33<14:13, 11.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 11%|▊      | 9/84 [01:41<12:39, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 12%|▋     | 10/84 [01:52<13:06, 10.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 13%|▊     | 11/84 [02:02<12:30, 10.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 14%|▊     | 12/84 [02:10<11:31,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 15%|▉     | 13/84 [02:22<12:16, 10.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 17%|█     | 14/84 [02:33<12:13, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 18%|█     | 15/84 [02:43<11:51, 10.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 19%|█▏    | 16/84 [02:57<12:52, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 20%|█▏    | 17/84 [03:06<12:07, 10.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 21%|█▎    | 18/84 [03:22<13:37, 12.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 23%|█▎    | 19/84 [03:31<12:16, 11.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 24%|█▍    | 20/84 [03:43<12:12, 11.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 25%|█▌    | 21/84 [03:55<12:11, 11.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 26%|█▌    | 22/84 [04:04<11:13, 10.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 27%|█▋    | 23/84 [04:19<12:25, 12.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 29%|█▋    | 24/84 [04:31<11:56, 11.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 30%|█▊    | 25/84 [04:40<11:08, 11.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 31%|█▊    | 26/84 [04:54<11:33, 11.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 32%|█▉    | 27/84 [05:08<11:52, 12.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 33%|██    | 28/84 [05:24<12:40, 13.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 35%|██    | 29/84 [05:35<11:55, 13.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 36%|██▏   | 30/84 [05:45<10:42, 11.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 16388049 e pred
 37%|██▏   | 31/84 [05:56<10:16, 11.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 38%|██▎   | 32/84 [06:03<08:52, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 39%|██▎   | 33/84 [06:18<09:52, 11.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 40%|██▍   | 34/84 [06:29<09:32, 11.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 42%|██▌   | 35/84 [06:43<09:57, 12.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 43%|██▌   | 36/84 [06:52<09:01, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 44%|██▋   | 37/84 [07:04<08:59, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 45%|██▋   | 38/84 [07:14<08:32, 11.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 46%|██▊   | 39/84 [07:24<08:00, 10.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 48%|██▊   | 40/84 [07:33<07:32, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 49%|██▉   | 41/84 [07:45<07:41, 10.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 50%|███   | 42/84 [07:56<07:41, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 51%|███   | 43/84 [08:10<08:07, 11.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 52%|███▏  | 44/84 [08:23<08:08, 12.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 54%|███▏  | 45/84 [08:31<07:07, 10.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 55%|███▎  | 46/84 [08:46<07:41, 12.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 56%|███▎  | 47/84 [09:02<08:06, 13.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 57%|███▍  | 48/84 [09:13<07:36, 12.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 58%|███▌  | 49/84 [09:29<08:00, 13.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 60%|███▌  | 50/84 [09:39<07:05, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 61%|███▋  | 51/84 [09:56<07:33, 13.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 62%|███▋  | 52/84 [10:09<07:13, 13.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 63%|███▊  | 53/84 [10:22<06:58, 13.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 64%|███▊  | 54/84 [10:37<06:53, 13.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 65%|███▉  | 55/84 [10:50<06:34, 13.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 67%|████  | 56/84 [10:59<05:41, 12.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 68%|████  | 57/84 [11:10<05:18, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 69%|████▏ | 58/84 [11:21<05:06, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 70%|████▏ | 59/84 [11:31<04:39, 11.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 71%|████▎ | 60/84 [11:46<04:51, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 31085942 e pred
 73%|████▎ | 61/84 [12:05<05:26, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 74%|████▍ | 62/84 [12:15<04:49, 13.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 75%|████▌ | 63/84 [12:25<04:17, 12.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 76%|████▌ | 64/84 [12:36<03:54, 11.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 77%|████▋ | 65/84 [12:47<03:37, 11.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 79%|████▋ | 66/84 [13:04<03:55, 13.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 80%|████▊ | 67/84 [13:13<03:23, 11.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 81%|████▊ | 68/84 [13:25<03:10, 11.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 82%|████▉ | 69/84 [13:36<02:55, 11.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 11849486 e pred
 83%|█████ | 70/84 [13:53<03:04, 13.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 85%|█████ | 71/84 [14:09<03:04, 14.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 86%|█████▏| 72/84 [14:22<02:45, 13.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 87%|█████▏| 73/84 [14:33<02:21, 12.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 88%|█████▎| 74/84 [14:43<02:00, 12.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 89%|█████▎| 75/84 [14:52<01:41, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 90%|█████▍| 76/84 [15:05<01:32, 11.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 92%|█████▌| 77/84 [15:16<01:21, 11.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 93%|█████▌| 78/84 [15:31<01:15, 12.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 94%|█████▋| 79/84 [15:41<00:57, 11.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 44885018 e pred
 95%|█████▋| 80/84 [16:01<00:57, 14.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 96%|█████▊| 81/84 [16:09<00:37, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 98%|█████▊| 82/84 [16:21<00:24, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 99%|█████▉| 83/84 [16:32<00:11, 11.88s/it]
100%|██████| 84/84 [16:41<00:00, 11.02s/it]

## Finishing Time: 09-11 12:18:03
= = = = = = = = = = = = = = = = = = = =
Done!
