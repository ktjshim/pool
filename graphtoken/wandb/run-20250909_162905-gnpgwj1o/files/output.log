= = = = = = = = = = = = = = = = = = = =
## Starting Time: 09-09 15:29:06
Namespace(dataset='huggingface', llm='Mistral-7B', llm_model_path='', seed=0, device='cuda:0', max_txt_length=512, max_ans_length=512, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_output_dim=4096, n_layers=2, gnn_type='SAGE', max_degree=300, num_epochs=2, batch_size=6, eval_batch_size=6, patience=2, lr=1e-05, wd=0.05, output_dir='output', grad_steps=4, name='node')

[Training Data] # Chain Samples 1536 (51.20)
[Data Split] # Train 3000  # Test 500
# Train 2400   # Val 600   # Test 500
Loading checkpoint shards: 100%|██████████████████████████████| 3/3 [00:01<00:00,  2.02it/s]
Finish loading pre-trained Mistral-7B model!
Trainable params 10492928 || all params 7252225024 || trainable% 0.14469
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(row_sum, -0.5).flatten()
/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:19: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:653.)
  return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))
Loading checkpoint from output/huggingface/Mistral-7B/SAGE_Epoch2_checkpoint_best.pth
  0%|                                            | 0/84 [00:00<?, ?it/s]/nas/home/ktshim/tool/pool/graphtoken/glm_node.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 15629367 e pred
[Test Error] Test-ID 21012060 e pred
  1%|▍                                   | 1/84 [00:19<26:22, 19.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 58719908 e pred
[Test Error] Test-ID 26514729 e pred
[Test Error] Test-ID 33517826 e pred
  2%|▊                                   | 2/84 [00:39<27:06, 19.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 23156734 e pred
[Test Error] Test-ID 32057897 e pred
  4%|█▎                                  | 3/84 [00:59<27:06, 20.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 92123547 e pred
[Test Error] Test-ID 17695157 e pred
[Test Error] Test-ID 28688470 e pred
  5%|█▋                                  | 4/84 [01:20<26:55, 20.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 16309918 e pred
[Test Error] Test-ID 96651044 e pred
[Test Error] Test-ID 26165291 e pred
  6%|██▏                                 | 5/84 [01:39<26:02, 19.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 26320836 e pred
[Test Error] Test-ID 12876313 e pred
[Test Error] Test-ID 99189811 e pred
  7%|██▌                                 | 6/84 [01:59<25:57, 19.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 17346258 e pred
[Test Error] Test-ID 23546976 e pred
  8%|███                                 | 7/84 [02:19<25:45, 20.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 20666150 e pred
 10%|███▍                                | 8/84 [02:37<24:30, 19.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 18395525 e pred
 11%|███▊                                | 9/84 [02:58<24:35, 19.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 41914644 e pred
[Test Error] Test-ID 50089396 e pred
 12%|████▏                              | 10/84 [03:18<24:37, 19.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 25388828 e pred
[Test Error] Test-ID 30327512 e pred
[Test Error] Test-ID 24781552 e pred
 13%|████▌                              | 11/84 [03:39<24:28, 20.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 25605833 e pred
 14%|█████                              | 12/84 [03:59<24:14, 20.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 28613625 e pred
[Test Error] Test-ID 24425303 e pred
[Test Error] Test-ID 13351081 e pred
[Test Error] Test-ID 17171654 e pred
 15%|█████▍                             | 13/84 [04:20<24:01, 20.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 13171761 e pred
[Test Error] Test-ID 20615183 e pred
 17%|█████▊                             | 14/84 [04:39<23:24, 20.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 68419360 e pred
[Test Error] Test-ID 32236593 e pred
[Test Error] Test-ID 93261718 e pred
 18%|██████▎                            | 15/84 [05:00<23:12, 20.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 69232823 e pred
[Test Error] Test-ID 28407458 e pred
[Test Error] Test-ID 88374923 e pred
 19%|██████▋                            | 16/84 [05:20<22:57, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 20%|███████                            | 17/84 [05:40<22:35, 20.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[Test Error] Test-ID 14414379 e pred
[Test Error] Test-ID 15165036 e pred
[Test Error] Test-ID 27315800 e pred
 21%|███████▌                           | 18/84 [06:00<22:17, 20.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Traceback (most recent call last):
  File "/nas/home/ktshim/tool/pool/graphtoken/main.py", line 210, in <module>
    id_list, predictions, requests = model.inference(batch, task_graph)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nas/home/ktshim/tool/pool/graphtoken/glm_node.py", line 163, in inference
    outputs = self.model.generate(
              ^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/generation/utils.py", line 2617, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/ktshim/.local/lib/python3.12/site-packages/transformers/generation/utils.py", line 3589, in _sample
    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
