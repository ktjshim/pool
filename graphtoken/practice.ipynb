{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186a5a7b",
   "metadata": {},
   "source": [
    "## 9/1\n",
    "- DiffPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01523df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import DenseSAGEConv, GCNConv, GATConv, TransformerConv, SAGEConv, GINConv\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "from torch_geometric.nn.dense import dense_diff_pool\n",
    "\n",
    "def build_conv(conv_type: str):\n",
    "    \"\"\"Return the specific gnn as`conv_type`\"\"\"\n",
    "    if conv_type == \"GCN\":\n",
    "        return GCNConv\n",
    "    elif conv_type == \"GIN\":\n",
    "        return lambda i, h: GINConv(\n",
    "            nn.Sequential(nn.Linear(i, h), nn.ReLU(), nn.Linear(h, h))\n",
    "        )\n",
    "    elif conv_type == \"GAT\":\n",
    "        return GATConv\n",
    "    elif conv_type == \"TransformerConv\":\n",
    "        return TransformerConv\n",
    "    elif conv_type == \"SAGE\":\n",
    "        return SAGEConv\n",
    "    elif conv_type == \"DenseSAGE\":\n",
    "        return DenseSAGEConv\n",
    "    else:\n",
    "        raise KeyError(\"GNN_TYPE can only be GAT, GCN, SAGE, GIN, and TransformerConv\")\n",
    "\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Neural Network Encoder that uses sparse graph convolutions.\n",
    "    It can be configured with different GNN layers (GCN, GAT, SAGE, etc.).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=1, gnn_type=\"DenseSAGEConv\", dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = build_conv(gnn_type)\n",
    "\n",
    "        self.gnn_type = gnn_type\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.act = nn.LeakyReLU()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if n_layers == 1:\n",
    "            self.conv_layers.append(conv(input_dim, output_dim))\n",
    "            self.bns = nn.ModuleList()\n",
    "        else:\n",
    "            self.conv_layers.append(conv(input_dim, hidden_dim))\n",
    "            for _ in range(n_layers - 2):\n",
    "                self.conv_layers.append(conv(hidden_dim, hidden_dim))\n",
    "            self.conv_layers.append(conv(hidden_dim, output_dim))\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(n_layers-1)])\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.conv_layers:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, graph_conv in enumerate(self.conv_layers[:-1]):\n",
    "            x = graph_conv(x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = self.act(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            \n",
    "        node_emb = self.conv_layers[-1](x, edge_index)\n",
    "        return node_emb\n",
    "\n",
    "\n",
    "class DiffPoolGNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A Differentiable Pooling (DiffPool) layer that uses a GNNEncoder to learn\n",
    "    node embeddings and cluster assignments. This module performs one level of pooling.\n",
    "    \n",
    "    It takes a sparse graph (or a batch of graphs) and returns a pooled, dense,\n",
    "    and coarsened graph representation, along with the link prediction and entropy\n",
    "    losses from the DiffPool operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of the input node features.\n",
    "            hidden_dim (int): Dimensionality of the hidden layers in the GNNs.\n",
    "            output_dim (int): Dimensionality of the output node features after pooling.\n",
    "            num_clusters (int): The number of clusters to pool the nodes into.\n",
    "            n_layers (int): The number of layers in the internal GNNEncoders.\n",
    "            gnn_type (str): The type of GNN convolution to use (e.g., \"GAT\", \"GCN\").\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.gnn_embed = GNNEncoder(args.gnn_in_dim, args.gnn_hidden_dim, args.gnn_output_dim, args.n_layers, args.gnn_type, args.dropout)\n",
    "\n",
    "        self.gnn_pool = GNNEncoder(args.gnn_in_dim, args.gnn_hidden_dim, args.num_clusters, args.n_layers, args.gnn_type, args.dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "\n",
    "        if batch is None:\n",
    "            batch = x.new_zeros(x.size(0), dtype=torch.long)\n",
    "            \n",
    "        x_embed = self.gnn_embed(x, edge_index)\n",
    "        s = self.gnn_pool(x, edge_index)\n",
    "        \n",
    "        s = F.softmax(s, dim=-1)\n",
    "        \n",
    "        x_embed_dense, mask = to_dense_batch(x_embed, batch)\n",
    "        s_dense, _ = to_dense_batch(s, batch)\n",
    "        adj_dense = to_dense_adj(edge_index, batch)\n",
    "        \n",
    "        # 3. Apply the Differentiable Pooling operation\n",
    "        x_pooled, adj_pooled, link_loss, entropy_loss = dense_diff_pool(\n",
    "            x=x_embed_dense,\n",
    "            adj=adj_dense,\n",
    "            s=s_dense,\n",
    "            mask=mask\n",
    "        )\n",
    "        \n",
    "        return x_pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b164769",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Args' object has no attribute 'num_clusters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph_tokenize \u001b[38;5;241m=\u001b[39m \u001b[43mDiffPoolGNNEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 97\u001b[0m, in \u001b[0;36mDiffPoolGNNEncoder.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_embed \u001b[38;5;241m=\u001b[39m GNNEncoder(args\u001b[38;5;241m.\u001b[39mgnn_in_dim, args\u001b[38;5;241m.\u001b[39mgnn_hidden_dim, args\u001b[38;5;241m.\u001b[39mgnn_output_dim, args\u001b[38;5;241m.\u001b[39mn_layers, args\u001b[38;5;241m.\u001b[39mgnn_type, args\u001b[38;5;241m.\u001b[39mdropout)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_pool \u001b[38;5;241m=\u001b[39m GNNEncoder(args\u001b[38;5;241m.\u001b[39mgnn_in_dim, args\u001b[38;5;241m.\u001b[39mgnn_hidden_dim, \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_clusters\u001b[49m, args\u001b[38;5;241m.\u001b[39mn_layers, args\u001b[38;5;241m.\u001b[39mgnn_type, args\u001b[38;5;241m.\u001b[39mdropout)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Args' object has no attribute 'num_clusters'"
     ]
    }
   ],
   "source": [
    "graph_tokenize = DiffPoolGNNEncoder(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3f90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c504eb6b",
   "metadata": {},
   "source": [
    "## 9/9 \n",
    "- centrality 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99732ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "import torch.nn as nn \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from gnn import DiffPoolGNNEncoder\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import init_random_state, load_tool, get_cur_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca4f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(row_sum, -0.5).flatten()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    dataset=\"huggingface\",\n",
    "    llm=\"Mistral-7B\",\n",
    "    seed=0,\n",
    "    device=\"cuda:0\",\n",
    "    max_txt_length=512,\n",
    "    max_ans_length=256,\n",
    "    gnn_in_dim=1024,\n",
    "    gnn_hidden_dim=1024,\n",
    "    gnn_output_dim=2560, # mistral-7b: 4096, codellama-13b: 5120, gpt-oss-20b: 2880, # gemma-3-4b-it: 2560\n",
    "    n_layers=2,\n",
    "    gnn_type=\"SAGE\",\n",
    "    num_epochs=4,\n",
    "    batch_size=6,\n",
    "    eval_batch_size=6,\n",
    "    patience=2,\n",
    "    lr=1e-5,\n",
    "    wd=0.05,\n",
    "    dropout=0.0,\n",
    "    num_clusters=10,\n",
    "    output_dir=\"output\",\n",
    "    grad_steps=4\n",
    ")\n",
    "device=\"cuda\"\n",
    "\n",
    "tool_texts, tool2index, index2tool, edge_index, _, adj_g = load_tool(dataset_name=args.dataset)\n",
    "\n",
    "task_graph = Data(x=torch.FloatTensor(np.load(f\"process/{args.dataset}.npy\")), edge_index=edge_index).to(device)\n",
    "\n",
    "out_degree = degree(task_graph.edge_index[0], dtype=torch.long).to(device)\n",
    "in_degree = degree(task_graph.edge_index[1], dtype=torch.long).to(device)\n",
    "task_graph.out_degree = out_degree\n",
    "task_graph.in_degree = in_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35210db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 12, 12, 12, 12, 12,  0, 13, 13, 13,  9, 13, 10,  0, 12, 12,  9,  9,\n",
       "         3, 13,  2, 13,  9], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_graph.out_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc0ad37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13, 13, 13, 13, 13, 14,  5,  5,  5,  4,  5, 14, 14, 18, 18,  4,  4,\n",
       "        14,  2,  1,  2, 18], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_graph.in_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff489a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_encoder = nn.Embedding(23, args.gnn_hidden_dim, padding_idx=0).to(device)\n",
    "out_degree_encoder = nn.Embedding(23, args.gnn_hidden_dim, padding_idx=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc7ca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3691,  1.0561,  0.2520,  ..., -1.1737,  1.5478, -1.0749],\n",
       "        [ 1.3691,  1.0561,  0.2520,  ..., -1.1737,  1.5478, -1.0749],\n",
       "        [ 1.3691,  1.0561,  0.2520,  ..., -1.1737,  1.5478, -1.0749],\n",
       "        ...,\n",
       "        [ 1.2557,  0.1254, -0.3001,  ...,  0.4116, -0.0029,  0.3576],\n",
       "        [-2.0151,  0.9396, -1.0036,  ...,  0.8895, -0.8732,  1.0593],\n",
       "        [-0.3082, -1.2320, -0.4838,  ...,  0.2067,  1.3552, -0.9189]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_degree_encoder(task_graph.in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0749a2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8714, -0.7760, -0.1889,  ..., -1.5068, -2.3798,  0.1450],\n",
       "        [ 0.8714, -0.7760, -0.1889,  ..., -1.5068, -2.3798,  0.1450],\n",
       "        [ 0.8714, -0.7760, -0.1889,  ..., -1.5068, -2.3798,  0.1450],\n",
       "        ...,\n",
       "        [-0.7180,  0.1857, -0.3028,  ..., -1.2868,  0.1780, -0.8984],\n",
       "        [ 0.1651, -0.8536, -0.1650,  ..., -0.0422,  0.7681,  0.0489],\n",
       "        [ 0.1489, -0.0531, -0.0896,  ...,  0.0845,  0.0602,  1.2075]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_degree_encoder(task_graph.out_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321a91b",
   "metadata": {},
   "source": [
    "### diffpool encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16788893",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_conv() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDiffPoolGNNEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_in_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_output_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgnn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_type\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/home/ktshim/tool/pool/graphtoken/gnn.py:87\u001b[0m, in \u001b[0;36mDiffPoolGNNEncoder.__init__\u001b[0;34m(self, input_dim, hidden_dim, output_dim, num_nodes, n_layers, gnn_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nodes_per_layer[i]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# GNN for learning node embeddings\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m gnn_embed \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# GNN for learning the cluster assignment matrix\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# The output dimension of gnn_assign must match the number of clusters.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m gnn_assign \u001b[38;5;241m=\u001b[39m build_conv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_type, current_dim, num_clusters)\n",
      "\u001b[0;31mTypeError\u001b[0m: build_conv() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "graph_tokenizer = DiffPoolGNNEncoder(\n",
    "        input_dim=args.gnn_in_dim, \n",
    "        hidden_dim=args.gnn_hidden_dim, \n",
    "        output_dim=args.gnn_output_dim, \n",
    "        num_nodes=23,\n",
    "        n_layers=args.n_layers, \n",
    "        gnn_type=args.gnn_type\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb94704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(23*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafdf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3546a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade5c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9cacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a776339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6f13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58067e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19776247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6202fa5",
   "metadata": {},
   "source": [
    "## 9/5 \n",
    "- graphtoken node version main practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f530e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Data] # Chain Samples 1509 (50.30)\n",
      "[Data Split] # Train 3000  # Test 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n",
      "/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(row_sum, -0.5).flatten()\n",
      "/nas/home/ktshim/tool/pool/graphtoken/../utils/datautil.py:19: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:654.)\n",
      "  return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading pre-trained Mistral-7B model!\n",
      "10,492,928\n",
      "7,252,225,024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1600 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from graph_llm import GraphToken \n",
    "from glm_node import GraphToken\n",
    "from plan_dataset import TaskPlanningDataset\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import init_random_state, load_tool, get_cur_time\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    dataset=\"huggingface\",\n",
    "    llm=\"Mistral-7B\",\n",
    "    seed=0,\n",
    "    device=\"cuda:0\",\n",
    "    max_txt_length=512,\n",
    "    max_ans_length=256,\n",
    "    gnn_in_dim=1024,\n",
    "    gnn_hidden_dim=1024,\n",
    "    gnn_output_dim=2560, # mistral-7b: 4096, codellama-13b: 5120, gpt-oss-20b: 2880, # gemma-3-4b-it: 2560\n",
    "    n_layers=2,\n",
    "    gnn_type=\"SAGE\",\n",
    "    num_epochs=4,\n",
    "    batch_size=6,\n",
    "    eval_batch_size=6,\n",
    "    patience=2,\n",
    "    lr=1e-5,\n",
    "    wd=0.05,\n",
    "    output_dir=\"output\",\n",
    "    grad_steps=4\n",
    ")\n",
    "\n",
    "path_mapping = {\n",
    "    \"CodeLlama-13B\": \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    \"Mistral-7B\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"CodeLlama-7B\": \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    \"Vicuna-13B\": \"lmsys/vicuna-13b-v1.5\",\n",
    "    \"gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "    \"gemma-3-270m-it\": \"google/gemma-3-270m-it\",\n",
    "    \"gemma-3-4b-it\": \"google/gemma-3-4b-it\"\n",
    "}\n",
    "\n",
    "\n",
    "gnn_hidden_mapping = {\"CodeLlama-13B\": 5120, \"Mistral-7B\": 4096, \"Vicuna-13B\": 5120, \"CodeLlama-7B\": 4096, \"gpt-oss-20b\": 2880, \"gemma-3-4b-it\": 2560}\n",
    "args.llm_model_path = path_mapping[args.llm]\n",
    "args.gnn_output_dim = gnn_hidden_mapping[args.llm]\n",
    "\n",
    "\n",
    "plan_dataset = TaskPlanningDataset(args.dataset)\n",
    "\n",
    "train_ids = plan_dataset.idxes_split[\"train\"]\n",
    "test_ids = plan_dataset.idxes_split[\"test\"]\n",
    "\n",
    "train_dataset = [plan_dataset[i] for i in train_ids[: int(0.8 * len(train_ids))]]\n",
    "eval_dataset = [plan_dataset[i] for i in train_ids[int(0.8 * len(train_ids)) :]]\n",
    "test_dataset = [plan_dataset[i] for i in test_ids]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, drop_last=True, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(eval_dataset, batch_size=args.eval_batch_size, drop_last=False, pin_memory=True, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.eval_batch_size, drop_last=False, pin_memory=True, shuffle=False)\n",
    "\n",
    "model = GraphToken(args)\n",
    "params = [p for _, p in model.named_parameters() if p.requires_grad]\n",
    "trainable_params, all_params = model.print_trainable_params()\n",
    "print(f\"{trainable_params:,}\")\n",
    "print(f\"{all_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "device='cuda:0'\n",
    "tool_texts, tool2index, index2tool, edge_index, _, adj_g = load_tool(dataset_name=args.dataset)\n",
    "task_graph = Data(x=torch.FloatTensor(np.load(f\"process/{args.dataset}.npy\")), edge_index=edge_index).to(device)\n",
    "\n",
    "num_training_steps = args.num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "num_training_steps = args.num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "best_val_loss = float('inf')\n",
    "model.model.gradient_checkpointing_enable() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755c0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b376c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/ktshim/tool/GNN4TaskPlan/GraphToken/graph_llm_node.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast(dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "loss = model(samples, task_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ecf268d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9793, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd898d71",
   "metadata": {},
   "source": [
    "### forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372d36b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([ 622, 2236,  826,  756, 2314,   96]),\n",
       " 'origin_id': ['26775635',\n",
       "  '17121963',\n",
       "  '14809338',\n",
       "  '30836617',\n",
       "  '86015657',\n",
       "  '20773083'],\n",
       " 'request': ['# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image \\'example.jpg\\' of a cat, but I want to change its color to green and then find out the category of the edited image with a spoken result.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have a long article about the history of photography and would like to get a shorter summary of it. Here\\'s the text: \\'Photography has come a long way since its invention in the early 19th century. From the first photographs taken using a camera obscura to the invention of digital cameras, the evolution of photography has been a fascinating journey. The Daguerreotype, the first widely used photographic process, transformed the way people saw the world around them and captured permanent images. Throughout the 20th century, the development of color film and instant cameras brought photography to the masses. In recent years, digital cameras and smartphones have enabled anyone to become a photographer, and social media platforms have changed the way we share our images. From the first flash of light to the click of a shutter, the story of photography is a captivating tale of innovation and reinvention.\\'\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I want to know how similar the sentences \\'The big brown dog jumped over the fence\\' and \\'The large brown canine hopped across the barrier\\' are.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image of a historical document \\'example.jpg\\'. I want to enhance its quality, extract named entities from it, and then listen to an audio version of these entities.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image \\'example.jpg\\' which contains several objects. I want to know which object is the largest in the image. After identifying the largest object, please change its color to red. Finally, enhance the overall quality of the modified image.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image (example.jpg) that I would like to enhance by matching the characteristics of a target domain, and then estimate the depth of objects within it. I need a summarised textual description of the whole processed image and an audio output based on the summary.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:'],\n",
       " 'label': ['{\"task_steps\": [\"Step 1: Modify the input image to match the given text description\", \"Step 2: Classify the edited image\", \"Step 3: Convert the classification result into speech\"], \"task_nodes\": [\"Image Classification\", \"Image Editing\", \"Text-to-Speech\"], \"task_links\": [{\"source\": \"Image Classification\", \"target\": \"Text-to-Speech\"}, {\"source\": \"Image Editing\", \"target\": \"Image Classification\"}]}',\n",
       "  '{\"task_steps\": [\"Step 1: Summarize the user-provided text using the Summarization tool\"], \"task_nodes\": [\"Summarization\"], \"task_links\": []}',\n",
       "  '{\"task_steps\": [\"Step 1: Find the similarity between two given sentences\"], \"task_nodes\": [\"Sentence Similarity\"], \"task_links\": []}',\n",
       "  '{\"task_steps\": [\"Step 1: Enhance the input image \\'example.jpg\\' using the Image-to-Image model.\", \"Step 2: Convert the enhanced image to text using the Image-to-Text model.\", \"Step 3: Extract named entities from the text using the Token Classification model.\", \"Step 4: Generate audio from the extracted entities using the Text-to-Speech model.\", \"Step 5: Transcribe the generated audio back into text using the Automatic Speech Recognition model.\"], \"task_nodes\": [\"Image-to-Image\", \"Image-to-Text\", \"Token Classification\", \"Text-to-Speech\"], \"task_links\": [{\"source\": \"Image-to-Image\", \"target\": \"Image-to-Text\"}, {\"source\": \"Image-to-Text\", \"target\": \"Token Classification\"}, {\"source\": \"Token Classification\", \"target\": \"Text-to-Speech\"}]}',\n",
       "  '{\"task_steps\": [\"Step 1: Answer a question about the image \\'example.jpg\\' with the given text.\", \"Step 2: Modify the image \\'example.jpg\\' based on the answer from Step 1.\", \"Step 3: Enhance the modified image from Step 2.\"], \"task_nodes\": [\"Visual Question Answering\", \"Image Editing\", \"Image-to-Image\"], \"task_links\": [{\"source\": \"Visual Question Answering\", \"target\": \"Image Editing\"}, {\"source\": \"Image Editing\", \"target\": \"Image-to-Image\"}]}',\n",
       "  '{\"task_steps\": [\"Step 1: Apply Image-to-Image manipulation on the user\\'s example.jpg to match the characteristics of the target domain.\", \"Step 2: Use Depth Estimation to predict the depth of objects within the manipulated image.\", \"Step 3: Convert the depth-annotated image to text by using the Image-to-Text tool.\", \"Step 4: Summarize the text retrieved from the Image-to-Text tool.\", \"Step 5: Generate an audio output using the Text-to-Speech tool from the summarized text.\"], \"task_nodes\": [\"Depth Estimation\", \"Image-to-Image\", \"Image-to-Text\", \"Summarization\", \"Text-to-Speech\"], \"task_links\": [{\"source\": \"Depth Estimation\", \"target\": \"Image-to-Text\"}, {\"source\": \"Image-to-Image\", \"target\": \"Depth Estimation\"}, {\"source\": \"Image-to-Text\", \"target\": \"Summarization\"}, {\"source\": \"Summarization\", \"target\": \"Text-to-Speech\"}]}']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "586e63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOS = '<s>[INST]'\n",
    "# EOS_USER = '[/INST]'\n",
    "# EOS = '</s>'\n",
    "# IGNORE_INDEX = -100 \n",
    "\n",
    "\n",
    "# encode prompts, user requests, and labels \n",
    "requests = model.tokenizer(samples[\"request\"], add_special_tokens=False)\n",
    "labels = model.tokenizer(samples[\"label\"], add_special_tokens=False)\n",
    "\n",
    "# encode special tokens\n",
    "eos_tokens = model.tokenizer(model.EOS, add_special_tokens=False)\n",
    "eos_user_tokens = model.tokenizer(model.EOS_USER, add_special_tokens=False)\n",
    "bos_tokens = model.tokenizer(model.BOS, add_special_tokens=False, return_tensors='pt').input_ids[0]\n",
    "bos_embeds = model.word_embedding(bos_tokens.to(model.device))\n",
    "pad_embeds = model.word_embedding(torch.tensor(model.tokenizer.pad_token_id).to(model.device)).unsqueeze(0)\n",
    "\n",
    "batch_size = len(samples['id'])\n",
    "# encode graphs \n",
    "node_embeds = model.encode_task_graph(task_graph, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19bea67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 3469, 464, 7476, 28723, 11692, 28742, 302, 264, 5255, 28725, 562, 315, 947, 298, 2268, 871, 3181, 298, 5344, 304, 868, 1300, 575, 272, 8011, 302, 272, 19527, 3469, 395, 264, 14382, 1204, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 264, 1043, 5447, 684, 272, 3340, 302, 19824, 304, 682, 737, 298, 625, 264, 19367, 14060, 302, 378, 28723, 4003, 28742, 28713, 272, 2245, 28747, 464, 3330, 322, 5064, 659, 1567, 264, 1043, 1069, 1854, 871, 21823, 297, 272, 2935, 28705, 28740, 28774, 362, 5445, 28723, 3672, 272, 907, 17806, 3214, 1413, 264, 7555, 16502, 2614, 298, 272, 21823, 302, 7153, 18713, 28725, 272, 10195, 302, 19824, 659, 750, 264, 23069, 8123, 28723, 415, 384, 357, 3807, 267, 21475, 28725, 272, 907, 12575, 1307, 9180, 294, 1759, 28725, 18252, 272, 1069, 905, 2672, 272, 1526, 1401, 706, 304, 13382, 13089, 6203, 28723, 23501, 272, 28705, 28750, 28734, 362, 5445, 28725, 272, 4099, 302, 3181, 2966, 304, 10990, 18713, 4248, 19824, 298, 272, 17536, 28723, 560, 5391, 1267, 28725, 7153, 18713, 304, 7455, 18624, 506, 9651, 3637, 298, 2727, 264, 24159, 28725, 304, 2809, 4077, 14926, 506, 4648, 272, 1069, 478, 4098, 813, 6203, 28723, 3672, 272, 907, 10745, 302, 2061, 298, 272, 6046, 302, 264, 6409, 360, 28725, 272, 2838, 302, 19824, 349, 264, 4286, 449, 1077, 15642, 302, 16863, 304, 11523, 6254, 1815, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 947, 298, 873, 910, 3684, 272, 23748, 464, 1014, 2032, 9060, 3914, 14949, 754, 272, 17210, 28742, 304, 464, 1014, 2475, 9060, 541, 473, 4654, 1973, 2673, 272, 19644, 28742, 460, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 3469, 302, 264, 10578, 3248, 464, 7476, 28723, 11692, 4135, 315, 947, 298, 11976, 871, 4045, 28725, 9131, 5160, 19810, 477, 378, 28725, 304, 868, 7105, 298, 396, 10466, 2751, 302, 1167, 19810, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 3469, 464, 7476, 28723, 11692, 28742, 690, 5876, 2856, 6697, 28723, 315, 947, 298, 873, 690, 1928, 349, 272, 7639, 297, 272, 3469, 28723, 2530, 21653, 272, 7639, 1928, 28725, 4665, 2268, 871, 3181, 298, 2760, 28723, 8126, 28725, 11976, 272, 7544, 4045, 302, 272, 11452, 3469, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 3469, 325, 7476, 28723, 11692, 28731, 369, 315, 682, 737, 298, 11976, 486, 11623, 272, 15559, 302, 264, 2718, 7966, 28725, 304, 868, 11679, 272, 8478, 302, 6697, 2373, 378, 28723, 315, 927, 264, 18062, 2458, 2245, 840, 5436, 302, 272, 2894, 16244, 3469, 304, 396, 10466, 3825, 2818, 356, 272, 14060, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 3813, 1575, 272, 2787, 3469, 298, 2918, 272, 2078, 2245, 5436, 548, 345, 9977, 28705, 28750, 28747, 4950, 1575, 272, 19527, 3469, 548, 345, 9977, 28705, 28770, 28747, 16186, 272, 16776, 1204, 778, 8666, 8883, 345, 5553, 28730, 12333, 1264, 7367, 4176, 4950, 2500, 548, 345, 4176, 2690, 4328, 548, 345, 1874, 28733, 532, 28733, 24812, 5295, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 4176, 4950, 2500, 548, 345, 3731, 1264, 345, 1874, 28733, 532, 28733, 24812, 5295, 7706, 9830, 1394, 1264, 345, 4176, 2690, 4328, 548, 345, 3731, 1264, 345, 4176, 4950, 2500, 17395, 9205], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 6927, 3479, 653, 272, 2188, 28733, 11951, 1932, 2245, 1413, 272, 6927, 3479, 1837, 3921, 8883, 345, 5553, 28730, 12333, 1264, 7367, 9881, 3479, 1837, 8883, 345, 5553, 28730, 17052, 1264, 3980, 28752], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 8769, 272, 3684, 472, 1444, 989, 2078, 23748, 8883, 345, 5553, 28730, 12333, 1264, 7367, 26968, 636, 24232, 472, 8883, 345, 5553, 28730, 17052, 1264, 3980, 28752], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 1618, 28716, 617, 272, 2787, 3469, 464, 7476, 28723, 11692, 28742, 1413, 272, 9833, 28733, 532, 28733, 4176, 2229, 9191, 345, 9977, 28705, 28750, 28747, 16186, 272, 21733, 3469, 298, 2245, 1413, 272, 9833, 28733, 532, 28733, 1874, 2229, 9191, 345, 9977, 28705, 28770, 28747, 1529, 2107, 5160, 19810, 477, 272, 2245, 1413, 272, 16625, 4950, 2500, 2229, 9191, 345, 9977, 28705, 28781, 28747, 26075, 10466, 477, 272, 25081, 19810, 1413, 272, 7379, 28733, 532, 28733, 24812, 5295, 2229, 9191, 345, 9977, 28705, 28782, 28747, 4335, 28717, 5748, 272, 7138, 10466, 852, 778, 2245, 1413, 272, 15939, 1711, 8819, 5295, 3523, 3159, 685, 2229, 611, 1181, 345, 5553, 28730, 12333, 1264, 7367, 4176, 28733, 532, 28733, 4176, 548, 345, 4176, 28733, 532, 28733, 1874, 548, 345, 3856, 4950, 2500, 548, 345, 1874, 28733, 532, 28733, 24812, 5295, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 4176, 28733, 532, 28733, 4176, 548, 345, 3731, 1264, 345, 4176, 28733, 532, 28733, 1874, 7706, 9830, 1394, 1264, 345, 4176, 28733, 532, 28733, 1874, 548, 345, 3731, 1264, 345, 3856, 4950, 2500, 7706, 9830, 1394, 1264, 345, 3856, 4950, 2500, 548, 345, 3731, 1264, 345, 1874, 28733, 532, 28733, 24812, 5295, 17395, 9205], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 26307, 264, 2996, 684, 272, 3469, 464, 7476, 28723, 11692, 28742, 395, 272, 2078, 2245, 9191, 345, 9977, 28705, 28750, 28747, 3813, 1575, 272, 3469, 464, 7476, 28723, 11692, 28742, 2818, 356, 272, 4372, 477, 7268, 28705, 28740, 9191, 345, 9977, 28705, 28770, 28747, 1618, 28716, 617, 272, 11452, 3469, 477, 7268, 28705, 28750, 611, 1181, 345, 5553, 28730, 12333, 1264, 7367, 21551, 22478, 1094, 1616, 2131, 548, 345, 4176, 2690, 4328, 548, 345, 4176, 28733, 532, 28733, 4176, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 21551, 22478, 1094, 1616, 2131, 548, 345, 3731, 1264, 345, 4176, 2690, 4328, 7706, 9830, 1394, 1264, 345, 4176, 2690, 4328, 548, 345, 3731, 1264, 345, 4176, 28733, 532, 28733, 4176, 17395, 9205], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 23160, 9833, 28733, 532, 28733, 4176, 13313, 2677, 356, 272, 2188, 28742, 28713, 2757, 28723, 11692, 298, 2918, 272, 15559, 302, 272, 2718, 7966, 9191, 345, 9977, 28705, 28750, 28747, 5938, 3995, 362, 3978, 8258, 298, 6782, 272, 8478, 302, 6697, 2373, 272, 13313, 6432, 3469, 9191, 345, 9977, 28705, 28770, 28747, 16186, 272, 8478, 28733, 5886, 601, 3469, 298, 2245, 486, 1413, 272, 9833, 28733, 532, 28733, 1874, 3921, 9191, 345, 9977, 28705, 28781, 28747, 6927, 3479, 653, 272, 2245, 17913, 286, 477, 272, 9833, 28733, 532, 28733, 1874, 3921, 9191, 345, 9977, 28705, 28782, 28747, 26075, 396, 10466, 3825, 1413, 272, 7379, 28733, 532, 28733, 24812, 5295, 3921, 477, 272, 18062, 1332, 2245, 611, 1181, 345, 5553, 28730, 12333, 1264, 7367, 17603, 3978, 8258, 548, 345, 4176, 28733, 532, 28733, 4176, 548, 345, 4176, 28733, 532, 28733, 1874, 548, 345, 9881, 3479, 1837, 548, 345, 1874, 28733, 532, 28733, 24812, 5295, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 17603, 3978, 8258, 548, 345, 3731, 1264, 345, 4176, 28733, 532, 28733, 1874, 7706, 9830, 1394, 1264, 345, 4176, 28733, 532, 28733, 4176, 548, 345, 3731, 1264, 345, 17603, 3978, 8258, 7706, 9830, 1394, 1264, 345, 4176, 28733, 532, 28733, 1874, 548, 345, 3731, 1264, 345, 9881, 3479, 1837, 7706, 9830, 1394, 1264, 345, 9881, 3479, 1837, 548, 345, 3731, 1264, 345, 1874, 28733, 532, 28733, 24812, 5295, 17395, 9205]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [2], 'attention_mask': [1]}\n",
      "{'input_ids': [733, 28748, 16289, 28793, 28705], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "tensor([    1,   733, 16289, 28793, 28705])\n",
      "torch.Size([5, 4096])\n",
      "torch.Size([1, 4096])\n",
      "6\n",
      "torch.Size([23, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(requests)\n",
    "print(labels)\n",
    "\n",
    "print(eos_tokens)\n",
    "print(eos_user_tokens)\n",
    "print(bos_tokens)\n",
    "print(bos_embeds.shape)\n",
    "print(pad_embeds.shape)\n",
    "print(batch_size)\n",
    "print(node_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76c9e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs_embeds = []\n",
    "batch_attention_masks = []\n",
    "batch_label_input_ids = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "\n",
    "    label_input_ids = labels.input_ids[i][:model.max_new_tokens] + eos_tokens.input_ids \n",
    "    input_ids = requests.input_ids[i][:model.max_txt_len] + eos_user_tokens.input_ids + label_input_ids\n",
    "\n",
    "    input_embeds = model.word_embedding(torch.tensor(input_ids).to(model.device))\n",
    "    input_embeds = torch.cat([bos_embeds, node_embeds, input_embeds], dim=0)\n",
    "    \n",
    "    batch_inputs_embeds.append(input_embeds)\n",
    "    \n",
    "    num_graph_tokens = node_embeds.shape[0]\n",
    "    seq_len = input_embeds.shape[0]\n",
    "    mask = torch.tril(torch.ones((seq_len, seq_len), device=model.device))\n",
    "    graph_end_idx = len(bos_tokens) + num_graph_tokens\n",
    "    mask[:graph_end_idx, :graph_end_idx] = 1\n",
    "    batch_attention_masks.append(mask)\n",
    "\n",
    "    label_input_ids = [model.IGNORE_INDEX] * (input_embeds.shape[0] - len(label_input_ids)) + label_input_ids\n",
    "    batch_label_input_ids.append(label_input_ids)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c974a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([520, 4096])\n",
      "torch.Size([520, 520])\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "print(batch_inputs_embeds[0].shape)\n",
    "print(batch_attention_masks[0].shape)\n",
    "print(len(batch_label_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61bc3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = max([x.shape[0] for x in batch_inputs_embeds])\n",
    "# added -----------------\n",
    "attention_mask = torch.zeros(batch_size, max_length, max_length, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d29af0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "84\n",
      "230\n",
      "51\n",
      "116\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    pad_length = max_length - batch_inputs_embeds[i].shape[0]\n",
    "    print(pad_length)\n",
    "    \n",
    "    batch_inputs_embeds[i] = torch.cat([pad_embeds.repeat(pad_length, 1), batch_inputs_embeds[i]])\n",
    "    \n",
    "    # changed -------------\n",
    "    attention_mask[i, pad_length:, pad_length:] = batch_attention_masks[i]\n",
    "    # ----------------------\n",
    "    \n",
    "    batch_label_input_ids[i] = [model.IGNORE_INDEX] * pad_length + batch_label_input_ids[i]\n",
    "\n",
    "input_embeds = torch.stack(batch_inputs_embeds, dim=0).to(model.model.device, model.model.dtype)\n",
    "# changed -------------------------------------\n",
    "# attention_mask = torch.tensor(batch_attention_mask).to(model.model.device)\n",
    "attention_mask = attention_mask.unsqueeze(1).to(model.model.device, model.model.dtype) # added dtype\n",
    "# ------------------------------------------\n",
    "label_input_ids = torch.tensor(batch_label_input_ids).to(model.model.device)\n",
    "batch_label_input_ids.append(label_input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa8a2670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 673, 4096]) torch.float16\n",
      "torch.Size([6, 673]) torch.int64\n",
      "torch.Size([6, 1, 673, 673]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(input_embeds.shape, input_embeds.dtype)\n",
    "print(label_input_ids.shape, label_input_ids.dtype)\n",
    "print(attention_mask.shape, attention_mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bf4dd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2db3834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16186,   272, 16776,  1204,   778,  8666,  8883,   345,  5553, 28730,\n",
       "        12333,  1264,  7367,  4176,  4950,  2500,   548,   345,  4176,  2690,\n",
       "         4328,   548,   345,  1874, 28733,   532, 28733, 24812,  5295,  8883,\n",
       "          345,  5553, 28730, 17052,  1264,   733,  6799,  1394,  1264,   345,\n",
       "         4176,  4950,  2500,   548,   345,  3731,  1264,   345,  1874, 28733,\n",
       "          532, 28733, 24812,  5295,  7706,  9830,  1394,  1264,   345,  4176,\n",
       "         2690,  4328,   548,   345,  3731,  1264,   345,  4176,  4950,  2500,\n",
       "        17395,  9205,     2], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_input_ids[0][600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d4ab225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'steps\": [\"Step 1: Modify the input image to match the given text description\", \"Step 2: Classify the edited image\", \"Step 3: Convert the classification result into speech\"], \"task_nodes\": [\"Image Classification\", \"Image Editing\", \"Text-to-Speech\"], \"task_links\": [{\"source\": \"Image Classification\", \"target\": \"Text-to-Speech\"}, {\"source\": \"Image Editing\", \"target\": \"Image Classification\"}]}</s>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(label_input_ids[0][565:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4de6ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.model(\n",
    "    inputs_embeds=input_embeds,\n",
    "    attention_mask=attention_mask,\n",
    "    return_dict=True,\n",
    "    labels=label_input_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c32d9e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d440b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([6, 515, 32000])\n",
      "DynamicCache(layers=[<transformers.cache_utils.DynamicLayer object at 0x7fef25392b20>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296490>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c2967c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c2969d0>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296340>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296970>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296e50>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296d60>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c6040>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c65e0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c62b0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c6730>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c6a60>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c87c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8e20>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8460>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8af0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8220>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8940>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c84c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8580>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8910>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8970>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c82e0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8100>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8700>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c85b0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8190>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c81c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c88e0>, <transformers.cache_utils.DynamicLayer object at 0x7fef25392d30>, <transformers.cache_utils.DynamicLayer object at 0x7fef25392640>])\n"
     ]
    }
   ],
   "source": [
    "print(outputs['loss'])\n",
    "print(outputs['logits'].shape)\n",
    "print(outputs['past_key_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbb1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache(layers=[<transformers.cache_utils.DynamicLayer object at 0x7fef25392b20>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296490>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c2967c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c2969d0>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296340>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296970>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296e50>, <transformers.cache_utils.DynamicLayer object at 0x7fef4c296d60>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c6040>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c65e0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c62b0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c6730>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c6a60>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c87c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8e20>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8460>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8af0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8220>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8940>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c84c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8580>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8910>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8970>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c82e0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8100>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8700>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c85b0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c8190>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c81c0>, <transformers.cache_utils.DynamicLayer object at 0x7fef253c88e0>, <transformers.cache_utils.DynamicLayer object at 0x7fef25392d30>, <transformers.cache_utils.DynamicLayer object at 0x7fef25392640>])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70983721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "811e18e2",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e9985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a4d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n",
      "torch.Size([6, 524, 524])\n",
      "torch.Size([6, 524, 4096]) torch.float16\n",
      "torch.Size([6, 1, 524, 524]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "requests = model.tokenizer(samples[\"request\"], add_special_tokens=False)\n",
    "\n",
    "eos_user_tokens = model.tokenizer(model.EOS_USER, add_special_tokens=False)\n",
    "bos_tokens = model.tokenizer(model.BOS, add_special_tokens=False, return_tensors='pt').input_ids[0]\n",
    "bos_embeds = model.word_embedding(bos_tokens.to(model.device))\n",
    "pad_embeds = model.word_embedding(torch.tensor(model.tokenizer.pad_token_id).to(model.device)).unsqueeze(0)\n",
    "\n",
    "batch_size = len(samples[\"id\"])\n",
    "node_embeds = model.encode_task_graph(task_graph, batch_size)\n",
    "num_graph_tokens = node_embeds.shape[0]\n",
    "\n",
    "batch_inputs_embeds = []\n",
    "batch_attention_masks = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    input_ids = requests.input_ids[i][:model.max_txt_len] + eos_user_tokens.input_ids\n",
    "    input_embeds = model.word_embedding(torch.tensor(input_ids).to(model.model.device))\n",
    "    input_embeds = torch.cat([bos_embeds, node_embeds, input_embeds], dim=0)\n",
    "    batch_inputs_embeds.append(input_embeds)\n",
    "\n",
    "    seq_len = input_embeds.shape[0]\n",
    "    mask = torch.tril(torch.ones((seq_len, seq_len), device=model.device))\n",
    "    graph_end_idx = len(bos_tokens) + num_graph_tokens\n",
    "    mask[:graph_end_idx, :graph_end_idx] = 1\n",
    "    batch_attention_masks.append(mask)\n",
    "\n",
    "\n",
    "max_length = max([x.shape[0] for x in batch_inputs_embeds])\n",
    "print(max_length)\n",
    "\n",
    "attention_mask = torch.zeros(batch_size, max_length, max_length, device=model.device)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "\n",
    "for i in range(batch_size):\n",
    "    pad_length = max_length - batch_inputs_embeds[i].shape[0]\n",
    "    batch_inputs_embeds[i] = torch.cat([pad_embeds.repeat(pad_length, 1), batch_inputs_embeds[i]])\n",
    "    attention_mask[i, pad_length:, pad_length:] = batch_attention_masks[i]\n",
    "\n",
    "input_embeds = torch.stack(batch_inputs_embeds, dim=0).to(model.model.device, model.model.dtype)\n",
    "attention_mask = attention_mask.unsqueeze(1).to(model.model.device, model.model.dtype)\n",
    "\n",
    "print(input_embeds.shape, input_embeds.dtype)\n",
    "print(attention_mask.shape, attention_mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf7bc4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from output/huggingface/Mistral-7B/SAGE_Epoch4_checkpoint_best.pth\n"
     ]
    }
   ],
   "source": [
    "from ckpt import reload_best_model\n",
    "model = reload_best_model(model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27defe",
   "metadata": {},
   "source": [
    "#### efficient version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6992d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 524, 4096])\n",
      "torch.Size([6, 1, 524, 524])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeds.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5fc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # --- Corrected and Optimized Custom Generation Loop ---\n",
    "\n",
    "# # This assumes 'input_embeds' and the initial 'attention_mask' are defined.\n",
    "# batch_size = input_embeds.shape[0]\n",
    "\n",
    "# Store the generated tokens (not embeddings)\n",
    "generated_ids = torch.empty(batch_size, 0, dtype=torch.long, device=model.device)\n",
    "initial_sequence_length = input_embeds.shape[1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # --- 1. Priming Step (First Forward Pass) ---\n",
    "    # Create initial position_ids for the prompt\n",
    "    position_ids = torch.arange(\n",
    "        0, initial_sequence_length, dtype=torch.long, device=model.device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    # First forward pass to get the initial cache\n",
    "    outputs = model.model(\n",
    "        inputs_embeds=input_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "\n",
    "    # Get the first token to start the generation\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    generated_ids = torch.cat([generated_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # --- 2. Generation Loop (Using the Cache) ---\n",
    "    current_sequence_length = initial_sequence_length\n",
    "    for _ in range(512): \n",
    "        # Check for EOS token to stop generation\n",
    "        if (next_token == model.tokenizer.eos_token_id).all():\n",
    "            break\n",
    "\n",
    "        # Move to the next position\n",
    "        current_sequence_length += 1\n",
    "        \n",
    "        # **FIX 1: Create the correct position_ids for the new token**\n",
    "        # It's just the index of the last token in the sequence.\n",
    "        position_ids = torch.tensor(\n",
    "            [[current_sequence_length - 1]], device=model.device, dtype=torch.long\n",
    "        ).expand(batch_size, -1)\n",
    "\n",
    "        # **FIX 2: Create a simple attention mask for the new token**\n",
    "        # Shape: (batch_size, total_sequence_length). It's all ones because the new\n",
    "        # token can attend to all previous tokens in the cache.\n",
    "        attention_mask = torch.ones(\n",
    "            (batch_size, current_sequence_length), device=model.device, dtype=attention_mask.dtype\n",
    "        )\n",
    "\n",
    "        # Get the embedding for ONLY the last generated token\n",
    "        next_token_embeds = model.word_embedding(next_token).unsqueeze(1)\n",
    "\n",
    "        # --- Forward pass with cache, position_ids, and corrected mask ---\n",
    "        outputs = model.model(\n",
    "            inputs_embeds=next_token_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            use_cache=True,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "        # Update the cache\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Get the next token\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Append the new token to our results\n",
    "        generated_ids = torch.cat([generated_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "# Decode the final generated sequence\n",
    "pred = model.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc23dd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a28f1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\n",
      "2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\n",
      "3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\n",
      "4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\n",
      "\n",
      "# USER REQUEST #: Please find an answer to the question 'What is the Capital of France?' in the given example.jpg image and provide the answer as an audio file.\n",
      "Now please generate your result in a strict JSON format:\n",
      "# RESULT #: [/INST] 1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\n",
      "2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\n",
      "3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\n",
      "4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\n",
      "\n",
      "# USER REQUEST #: Please find an answer to the question 'What is the Capital of France?' in the given example.jpg image and provide the answer as an audio file.\n",
      "Now please generate your result in a strict JSON format:\n",
      "# RESULT #: [/INST] 1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\n",
      "2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must\n"
     ]
    }
   ],
   "source": [
    "print(pred[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "865c2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TASK LIST #:\n",
      "Token Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\n",
      "\n",
      "# GOAL #\n",
      "Please understand the user's request and generate task steps and task invocation graph to solve it.\n",
      "\n",
      "# REQUIREMENT #\n",
      "1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\n",
      "2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\n",
      "3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\n",
      "4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\n",
      "\n",
      "\n",
      "# USER REQUEST #: I have a text in Spanish about a famous painting, and I need it translated to English, summarized, and finally, I want to ask a question about the painting using the summarized information. The text is 'El Grito es una famosa pintura del artista noruego Edvard Munch. La obra representa una figura andrógina en un momento de angustia. El famoso puente en la pintura es el puente de la bahía de Oslo. Munch creó varias versiones de la pintura entre 1893 y 1910.' The image is 'example.jpg', and the question is 'What does the main figure in the painting represent?'\n",
      "Now please generate your result in a strict JSON format:\n",
      "# RESULT #:\n"
     ]
    }
   ],
   "source": [
    "print(samples['request'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabaf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f31c226",
   "metadata": {},
   "source": [
    "## gnn_llm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f6f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "# llm_model_path = \"google/gemma-3-270m\"\n",
    "# llm_model_path = \"openai/gpt-oss-20b\"\n",
    "llm_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(llm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2779a7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d797d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47b419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd7202a4",
   "metadata": {},
   "source": [
    "### mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ac496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n",
      "1\n",
      "left\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728c6eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "<s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.eos_token_id))\n",
    "print(tokenizer.decode(tokenizer.bos_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172c370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af15f0",
   "metadata": {},
   "source": [
    "### gpt-oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6db49549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "<|endoftext|>\n",
      "<|return|>\n",
      "<|constrain|>\n",
      "<|channel|>\n",
      "<|start|>\n",
      "<|end|>\n",
      "<|message|>\n",
      "<|reserved_200009|>\n",
      "<|reserved_200010|>\n",
      "<|reserved_200011|>\n",
      "<|call|>\n",
      "<|reserved_200013|>\n",
      "<|reserved_200014|>\n",
      "<|reserved_200015|>\n",
      "<|reserved_200016|>\n",
      "<|reserved_200017|>\n",
      "<|endofprompt|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.bos_token_id))\n",
    "print(tokenizer.decode(tokenizer.pad_token_id))\n",
    "print(tokenizer.decode(tokenizer.eos_token_id))\n",
    "print(tokenizer.decode(200003))\n",
    "# print(tokenizer.decode(200004))\n",
    "print(tokenizer.decode(200005))\n",
    "print(tokenizer.decode(200006))\n",
    "print(tokenizer.decode(200007))\n",
    "print(tokenizer.decode(200008))\n",
    "print(tokenizer.decode(200009))\n",
    "print(tokenizer.decode(200010))\n",
    "print(tokenizer.decode(200011))\n",
    "print(tokenizer.decode(200012))\n",
    "print(tokenizer.decode(200013))\n",
    "print(tokenizer.decode(200014))\n",
    "print(tokenizer.decode(200015))\n",
    "print(tokenizer.decode(200016))\n",
    "print(tokenizer.decode(200017))\n",
    "print(tokenizer.decode(200018))\n",
    "print(tokenizer.decode(200019))\n",
    "print(tokenizer.decode(200020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb0bf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(200006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2b5319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(200007))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06e3546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|message|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(200008))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6d3e7be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(199999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d3787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea5a17c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : <unk>\n",
      "1 : <s>\n",
      "2 : </s>\n",
      "3 : \u0000\n",
      "4 : \u0001\n",
      "5 : \u0002\n",
      "6 : \u0003\n",
      "7 : \u0004\n",
      "8 : \u0005\n",
      "9 : \u0006\n",
      "10 : \u0007\n",
      "11 :\n",
      "12 : \t\n",
      "13 : \n",
      "\n",
      "14 : \u000b\n",
      "15 : \f\n",
      "16 : \n",
      "17 : \u000e\n",
      "18 : \u000f\n",
      "19 : \u0010\n",
      "20 : \u0011\n",
      "21 : \u0012\n",
      "22 : \u0013\n",
      "23 : \u0014\n",
      "24 : \u0015\n",
      "25 : \u0016\n",
      "26 : \u0017\n",
      "27 : \u0018\n",
      "28 : \u0019\n",
      "29 : \u001a\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(f\"{i} : {tokenizer.decode(i)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74678aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f7913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0f8dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>[INST]'\n",
    "EOS_USER = '[/INST]'\n",
    "EOS = '</s>'\n",
    "\n",
    "eos_user_tokens = tokenizer(EOS_USER, add_special_tokens=False)\n",
    "bos_tokens = tokenizer(BOS, add_special_tokens=False)\n",
    "eos_tokens = tokenizer(EOS, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9058181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [49613, 34177, 236842], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [203, 236840, 34177, 236842], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [212], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "print(eos_user_tokens)\n",
    "print(bos_tokens)\n",
    "print(eos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "927118ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : <pad>\n",
      "1 : <eos>\n",
      "2 : <bos>\n",
      "3 : <unk>\n",
      "4 : <mask>\n",
      "5 : [multimodal]\n",
      "6 : <unused0>\n",
      "7 : <unused1>\n",
      "8 : <unused2>\n",
      "9 : <unused3>\n",
      "10 : <unused4>\n",
      "11 : <unused5>\n",
      "12 : <unused6>\n",
      "13 : <unused7>\n",
      "14 : <unused8>\n",
      "15 : <unused9>\n",
      "16 : <unused10>\n",
      "17 : <unused11>\n",
      "18 : <unused12>\n",
      "19 : <unused13>\n",
      "20 : <unused14>\n",
      "21 : <unused15>\n",
      "22 : <unused16>\n",
      "23 : <unused17>\n",
      "24 : <unused18>\n",
      "25 : <unused19>\n",
      "26 : <unused20>\n",
      "27 : <unused21>\n",
      "28 : <unused22>\n",
      "29 : <unused23>\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(f\"{i} : {tokenizer.decode(i)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaeebcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f921a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9dcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287514ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 733, 16289, 28793]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('[INST]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f0b7a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 28792, 16289, 28793]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<s>[INST]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d40f58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 28792, 16289, 28793]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS = '<s>[INST]'\n",
    "EOS_USER = '[/INST]'\n",
    "EOS = '</s>'\n",
    "\n",
    "tokenizer(BOS, add_special_tokens=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07eb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "645aec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Hello! Can you tell me about the weather in Hanam-si today? [/INST] Of course! The weather in Hanam-si, Gyeonggi-do is currently sunny with a high of 28°C.</s> [INST] That sounds great. What about tomorrow? [/INST]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you tell me about the weather in Hanam-si today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Of course! The weather in Hanam-si, Gyeonggi-do is currently sunny with a high of 28°C.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That sounds great. What about tomorrow?\"}\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e5c5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f94558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5e8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Hello! Can you tell me about the weather in Hanam-si today?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Of course! The weather in Hanam-si, Gyeonggi-do is currently sunny with a high of 28°C.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "That sounds great. What about tomorrow?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you tell me about the weather in Hanam-si today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Of course! The weather in Hanam-si, Gyeonggi-do is currently sunny with a high of 28°C.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That sounds great. What about tomorrow?\"}\n",
    "] \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814db180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a384262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85641818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-29\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Hello! Can you tell me about the weather in Hanam-si today?<|end|><|start|>assistant<|channel|>final<|message|>Of course! The weather in Hanam-si, Gyeonggi-do is currently sunny with a high of 28°C.<|end|><|start|>user<|message|>That sounds great. What about tomorrow?<|end|><|start|>assistant\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you tell me about the weather in Hanam-si today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Of course! The weather in Hanam-si, Gyeonggi-do is currently sunny with a high of 28°C.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That sounds great. What about tomorrow?\"}\n",
    "\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc58975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "<|endoftext|>\n",
      "<|return|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.bos_token_id))\n",
    "print(tokenizer.decode(tokenizer.pad_token_id))\n",
    "print(tokenizer.decode(tokenizer.eos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d997c39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0502fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>\n",
      "<|return|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "209acb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 91, 16730, 91, 29]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|pad|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "25081fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200006]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|start|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1bf18b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200007]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7d7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9156ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.76s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a helpful assistant\n",
      "\n",
      "What is the capital of France? [/INST] The capital city of France is Paris. Paris is one of the most famous cities in the world and is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. It is also home to many important cultural and artistic institutions. Paris is located in the northern part of France and is the country's most populous city.</s>\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "device=\"cuda:0\"\n",
    "# llm_model_path = \"google/gemma-3-4b-it\"\n",
    "# llm_model_path = \"openai/gpt-oss-20b\"\n",
    "llm_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "# processor = AutoProcessor.from_pretrained(llm_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_path, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": {\"text\": \"Hello! Can you explain what LLM is?\"}},\n",
    "# ]\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "    generation = generation[0]\n",
    "    \n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7575ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad079254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ce2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "device=\"cuda:0\"\n",
    "llm_model_path = \"google/gemma-3-4b-it\"\n",
    "# llm_model_path = \"openai/gpt-oss-20b\"\n",
    "# llm_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "# processor = AutoProcessor.from_pretrained(llm_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_path, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": {\"text\": \"Hello! Can you explain what LLM is?\"}},\n",
    "# ]\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who is the president of the United States?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "    generation = generation[0]\n",
    "    \n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e303072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"google/gemma-3-4b-it\".split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Who is the president of the United States?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "As of today, November 2, 2023, the President of the United States is **Joe Biden**. \n",
      "\n",
      "You can always find the most up-to-date information on the White House website: [https://www.whitehouse.gov/](https://www.whitehouse.gov/)<end_of_turn>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3c4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b63047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant\n",
      "\n",
      "What is the capital of France?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The capital of France is **Paris**. 🇫🇷\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "\n",
    "llm_model_path = \"google/gemma-3-12b-it\"\n",
    "# llm_model_path = \"openai/gpt-oss-20b\"\n",
    "# llm_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "# processor = AutoProcessor.from_pretrained(llm_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_path, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": {\"text\": \"Hello! Can you explain what LLM is?\"}},\n",
    "# ]\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "    generation = generation[0]\n",
    "    \n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00034577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "What is the capital of France?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The capital of France is **Paris**.\n",
      "\n",
      "\n",
      "\n",
      "It's also the largest city in France and a global center for art, fashion, gastronomy, and culture.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "    generation = generation[0]\n",
    "    \n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbce4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6d28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40930dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997c999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-30\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "<|end|><|start|>user<|message|>What is the capital of France?<|end|><|start|>assistant<|channel|>analysis<|message|>We need to answer: \"What is the capital of France?\" The answer: Paris. Provide concise answer.<|end|><|start|>assistant<|channel|>final<|message|>The capital of France is **Paris**.<|return|>\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "device=\"cuda:0\"\n",
    "# llm_model_path = \"google/gemma-3-4b-it\"\n",
    "llm_model_path = \"openai/gpt-oss-20b\"\n",
    "# llm_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "# processor = AutoProcessor.from_pretrained(llm_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_path, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "messages = messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=600, do_sample=False)\n",
    "    generation = generation[0]\n",
    "    \n",
    "decoded = tokenizer.decode(generation, skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6aeac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-29\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Who is the president of the United States?<|end|><|start|>assistant<|channel|>analysis<|message|>The user asks: \"Who is the president of the United States?\" This is a factual question. As of the current date, 2025-08-29, the president is Joe Biden? Wait, Joe Biden was president from 2021 to 2025. But as of 2025, the next election is in 2024. Joe Biden's term ends on January 20, 2025. The 2024 election will decide the next president. As of August 2025, the president would be the winner of the 2024 election. The 2024 election hasn't happened yet? Actually, the 2024 election is scheduled for November 5, 2024. So as of August 2025, the president would be the winner of that election. But we don't know the outcome. However, the user might be asking as of now. The current president is Joe Biden until January 20, 2025. But the user might be asking in general. The best answer: Joe Biden is the current president until January 20, 2025. If they want the current president as of 2025-08-29, it's the winner of the 2024 election. But we don't know. We can say: As of now, Joe Biden is the president until January 20, 2025. After that, the president will be the winner of the 2024 election. Alternatively, we can say: The president is Joe Biden. But we should be careful: The user might be expecting the answer \"Joe Biden\" or \"Joe Biden (Democrat)\". We can mention that the next president will be determined in 2024. So answer: Joe Biden.<|end|><|start|>assistant<|channel|>final<|message|>As of now, **Joe Biden** is the President of the United States. He will serve until the inauguration on **January 20, 2025**, when the winner of the 2024 presidential election will take office.<|return|>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f388755a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd6cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab7d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525906af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be356c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6618fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a74b0685",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9dfea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/libpyg.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/ktshim/anaconda3/envs/tool/lib/python3.9/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Data] # Chain Samples 1514 (50.47)\n",
      "[Data Split] # Train 3000  # Test 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
      "/nas/home/ktshim/tool/GNN4TaskPlan/GraphToken/../utils/datautil.py:24: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(row_sum, -0.5).flatten()\n",
      "/nas/home/ktshim/tool/GNN4TaskPlan/GraphToken/../utils/datautil.py:19: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:654.)\n",
      "  return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading pre-trained Mistral-7B model!\n",
      "10,492,928\n",
      "7,252,225,024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1600 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from torch.utils.data import DataLoader\n",
    "from graph_llm import GraphToken \n",
    "from plan_dataset import TaskPlanningDataset\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import init_random_state, load_tool, get_cur_time\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Args(\n",
    "    dataset=\"huggingface\",\n",
    "    llm=\"Mistral-7B\",\n",
    "    seed=0,\n",
    "    device=\"cuda:0\",\n",
    "    max_txt_length=512,\n",
    "    max_ans_length=256,\n",
    "    gnn_in_dim=1024,\n",
    "    gnn_hidden_dim=1024,\n",
    "    gnn_output_dim=2560, # mistral-7b: 4096, codellama-13b: 5120, gpt-oss-20b: 2880, # gemma-3-4b-it: 2560\n",
    "    n_layers=2,\n",
    "    gnn_type=\"SAGE\",\n",
    "    num_epochs=4,\n",
    "    batch_size=6,\n",
    "    eval_batch_size=6,\n",
    "    patience=2,\n",
    "    lr=1e-5,\n",
    "    wd=0.05,\n",
    "    output_dir=\"output\",\n",
    "    grad_steps=4\n",
    ")\n",
    "\n",
    "path_mapping = {\n",
    "    \"CodeLlama-13B\": \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "    \"Mistral-7B\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"CodeLlama-7B\": \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    \"Vicuna-13B\": \"lmsys/vicuna-13b-v1.5\",\n",
    "    \"gpt-oss-20b\": \"openai/gpt-oss-20b\",\n",
    "    \"gemma-3-270m-it\": \"google/gemma-3-270m-it\",\n",
    "    \"gemma-3-4b-it\": \"google/gemma-3-4b-it\"\n",
    "}\n",
    "\n",
    "\n",
    "gnn_hidden_mapping = {\"CodeLlama-13B\": 5120, \"Mistral-7B\": 4096, \"Vicuna-13B\": 5120, \"CodeLlama-7B\": 4096, \"gpt-oss-20b\": 2880, \"gemma-3-4b-it\": 2560}\n",
    "args.llm_model_path = path_mapping[args.llm]\n",
    "args.gnn_output_dim = gnn_hidden_mapping[args.llm]\n",
    "\n",
    "\n",
    "plan_dataset = TaskPlanningDataset(args.dataset)\n",
    "\n",
    "train_ids = plan_dataset.idxes_split[\"train\"]\n",
    "test_ids = plan_dataset.idxes_split[\"test\"]\n",
    "\n",
    "train_dataset = [plan_dataset[i] for i in train_ids[: int(0.8 * len(train_ids))]]\n",
    "eval_dataset = [plan_dataset[i] for i in train_ids[int(0.8 * len(train_ids)) :]]\n",
    "test_dataset = [plan_dataset[i] for i in test_ids]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, drop_last=True, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(eval_dataset, batch_size=args.eval_batch_size, drop_last=False, pin_memory=True, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.eval_batch_size, drop_last=False, pin_memory=True, shuffle=False)\n",
    "\n",
    "model = GraphToken(args)\n",
    "params = [p for _, p in model.named_parameters() if p.requires_grad]\n",
    "trainable_params, all_params = model.print_trainable_params()\n",
    "print(f\"{trainable_params:,}\")\n",
    "print(f\"{all_params:,}\")\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device='cuda:0'\n",
    "tool_texts, tool2index, index2tool, edge_index, _, adj_g = load_tool(dataset_name=args.dataset)\n",
    "task_graph = Data(x=torch.FloatTensor(np.load(f\"process/{args.dataset}.npy\")), edge_index=edge_index).to(device)\n",
    "\n",
    "\n",
    "num_training_steps = args.num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "best_val_loss = float('inf')\n",
    "model.model.gradient_checkpointing_enable() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c864dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_loader):\n",
    "    step = step\n",
    "    batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f485a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[23, 1024], edge_index=[2, 225])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69029f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([ 228,  810,  618, 1876,   98, 1049]),\n",
       " 'origin_id': ['14719993',\n",
       "  '24037587',\n",
       "  '10482344',\n",
       "  '12432470',\n",
       "  '64894887',\n",
       "  '11478569'],\n",
       " 'request': ['# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image \\'example.jpg\\' containing a scenery with a red boat. I would like to change the color of the boat to blue, and then segment the image, all while working with a French description. Please assist with the translation, editing, and segmentation tasks.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have two sentences: \\'The quick brown fox jumps over the lazy dog.\\' and \\'The fast brown fox leaps over the lazy canine.\\' I want to know how similar these two sentences are.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image called \\'example.jpg\\' and I want to create a depth map of the objects present in it. Can you help me with that?\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: Please convert the following text into speech: \\'In the age of AI, it is important for everyone to have a basic understanding of how algorithms work. Learning how to code can help people better understand the digital world.\\'\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an audio file named example.wav where I discussed some important points during a meeting. I would like to have a written transcription of the content in a text.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:',\n",
       "  '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: Please summarize the following text and then answer the question \\'What is the main topic of the text?\\' using the provided image \\'example.jpg\\'. Text: \\'Machine learning is a technique that employs algorithms and statistical models to enable computers to learn from and accomplish tasks through experience, rather than relying on explicit programming. This technology has contributed significantly to the development of artificial intelligence, and it has been applied in various fields such as natural language processing, computer vision, and speech recognition.\\'\\nNow please generate your result in a strict JSON format:\\n# RESULT #:'],\n",
       " 'label': ['{\"task_steps\": [\"Step 1: Translate the text description from English to French.\", \"Step 2: Edit the image according to the translated text description.\", \"Step 3: Perform image segmentation on the edited image.\"], \"task_nodes\": [\"Translation\", \"Image Editing\", \"Image Segmentation\"], \"task_links\": [{\"source\": \"Translation\", \"target\": \"Image Editing\"}, {\"source\": \"Image Editing\", \"target\": \"Image Segmentation\"}]}',\n",
       "  '{\"task_steps\": [\"Step 1: Determine the similarity between two sentences\"], \"task_nodes\": [\"Sentence Similarity\"], \"task_links\": []}',\n",
       "  '{\"task_steps\": [\"Step 1: Estimate the depth of objects in an image using Depth Estimation tool.\"], \"task_nodes\": [\"Depth Estimation\"], \"task_links\": []}',\n",
       "  '{\"task_steps\": [\"Step 1: Use Text-to-Speech tool to convert the user-specified text into speech\"], \"task_nodes\": [\"Text-to-Speech\"], \"task_links\": []}',\n",
       "  '{\"task_steps\": [\"Step 1: Convert the user provided audio file into text\"], \"task_nodes\": [\"Automatic Speech Recognition\"], \"task_links\": []}',\n",
       "  '{\"task_steps\": [\"Step 1: Summarize the provided text.\", \"Step 2: Use the provided image and the summary text from Step 1 to answer the question.\"], \"task_nodes\": [\"Summarization\", \"Visual Question Answering\"], \"task_links\": [{\"source\": \"Summarization\", \"target\": \"Visual Question Answering\"}]}']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b3c1",
   "metadata": {},
   "source": [
    "#### inside forward(self, samples, task_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a9d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': tensor([ 228,  810,  618, 1876,   98, 1049]), 'origin_id': ['14719993', '24037587', '10482344', '12432470', '64894887', '11478569'], 'request': ['# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image \\'example.jpg\\' containing a scenery with a red boat. I would like to change the color of the boat to blue, and then segment the image, all while working with a French description. Please assist with the translation, editing, and segmentation tasks.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:', '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have two sentences: \\'The quick brown fox jumps over the lazy dog.\\' and \\'The fast brown fox leaps over the lazy canine.\\' I want to know how similar these two sentences are.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:', '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an image called \\'example.jpg\\' and I want to create a depth map of the objects present in it. Can you help me with that?\\nNow please generate your result in a strict JSON format:\\n# RESULT #:', '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: Please convert the following text into speech: \\'In the age of AI, it is important for everyone to have a basic understanding of how algorithms work. Learning how to code can help people better understand the digital world.\\'\\nNow please generate your result in a strict JSON format:\\n# RESULT #:', '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: I have an audio file named example.wav where I discussed some important points during a meeting. I would like to have a written transcription of the content in a text.\\nNow please generate your result in a strict JSON format:\\n# RESULT #:', '# TASK LIST #:\\nToken Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\\n\\n# GOAL #\\nPlease understand the user\\'s request and generate task steps and task invocation graph to solve it.\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user\\'s request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\\n\\n# USER REQUEST #: Please summarize the following text and then answer the question \\'What is the main topic of the text?\\' using the provided image \\'example.jpg\\'. Text: \\'Machine learning is a technique that employs algorithms and statistical models to enable computers to learn from and accomplish tasks through experience, rather than relying on explicit programming. This technology has contributed significantly to the development of artificial intelligence, and it has been applied in various fields such as natural language processing, computer vision, and speech recognition.\\'\\nNow please generate your result in a strict JSON format:\\n# RESULT #:'], 'label': ['{\"task_steps\": [\"Step 1: Translate the text description from English to French.\", \"Step 2: Edit the image according to the translated text description.\", \"Step 3: Perform image segmentation on the edited image.\"], \"task_nodes\": [\"Translation\", \"Image Editing\", \"Image Segmentation\"], \"task_links\": [{\"source\": \"Translation\", \"target\": \"Image Editing\"}, {\"source\": \"Image Editing\", \"target\": \"Image Segmentation\"}]}', '{\"task_steps\": [\"Step 1: Determine the similarity between two sentences\"], \"task_nodes\": [\"Sentence Similarity\"], \"task_links\": []}', '{\"task_steps\": [\"Step 1: Estimate the depth of objects in an image using Depth Estimation tool.\"], \"task_nodes\": [\"Depth Estimation\"], \"task_links\": []}', '{\"task_steps\": [\"Step 1: Use Text-to-Speech tool to convert the user-specified text into speech\"], \"task_nodes\": [\"Text-to-Speech\"], \"task_links\": []}', '{\"task_steps\": [\"Step 1: Convert the user provided audio file into text\"], \"task_nodes\": [\"Automatic Speech Recognition\"], \"task_links\": []}', '{\"task_steps\": [\"Step 1: Summarize the provided text.\", \"Step 2: Use the provided image and the summary text from Step 1 to answer the question.\"], \"task_nodes\": [\"Summarization\", \"Visual Question Answering\"], \"task_links\": [{\"source\": \"Summarization\", \"target\": \"Visual Question Answering\"}]}']}\n"
     ]
    }
   ],
   "source": [
    "samples = batch.copy()\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d9b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TASK LIST #:\n",
      "Token Classification, Translation, Summarization, Question Answering, Conversational, Text Generation, Sentence Similarity, Tabular Classification, Object Detection, Image Classification, Image-to-Image, Image-to-Text, Text-to-Image, Text-to-Video, Visual Question Answering, Document Question Answering, Image Segmentation, Depth Estimation, Text-to-Speech, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Image Editing\n",
      "\n",
      "# GOAL #\n",
      "Please understand the user's request and generate task steps and task invocation graph to solve it.\n",
      "\n",
      "# REQUIREMENT #\n",
      "1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\n",
      "2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\n",
      "3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\n",
      "4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\n",
      "\n",
      "\n",
      "# USER REQUEST #: I have an image 'example.jpg' containing a scenery with a red boat. I would like to change the color of the boat to blue, and then segment the image, all while working with a French description. Please assist with the translation, editing, and segmentation tasks.\n",
      "Now please generate your result in a strict JSON format:\n",
      "# RESULT #:\n"
     ]
    }
   ],
   "source": [
    "print(samples['request'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b726342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1, 422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 3469, 464, 7476, 28723, 11692, 28742, 8707, 264, 752, 790, 28724, 395, 264, 2760, 9088, 28723, 315, 682, 737, 298, 2268, 272, 3181, 302, 272, 9088, 298, 5045, 28725, 304, 868, 10424, 272, 3469, 28725, 544, 1312, 2739, 395, 264, 4949, 5436, 28723, 5919, 6031, 395, 272, 13846, 28725, 19617, 28725, 304, 10424, 352, 9796, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [1, 422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 989, 23748, 28747, 464, 1014, 2936, 9060, 285, 1142, 461, 10575, 754, 272, 17898, 3914, 1815, 304, 464, 1014, 4102, 9060, 285, 1142, 462, 1882, 754, 272, 17898, 541, 473, 1815, 315, 947, 298, 873, 910, 3684, 1167, 989, 23748, 460, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [1, 422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 3469, 1987, 464, 7476, 28723, 11692, 28742, 304, 315, 947, 298, 2231, 264, 8478, 3341, 302, 272, 6697, 2169, 297, 378, 28723, 2418, 368, 1316, 528, 395, 369, 28804, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [1, 422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 5919, 6603, 272, 2296, 2245, 778, 8666, 28747, 464, 657, 272, 3595, 302, 16107, 28725, 378, 349, 2278, 354, 3376, 298, 506, 264, 6471, 6399, 302, 910, 18539, 771, 28723, 17504, 910, 298, 2696, 541, 1316, 905, 1873, 2380, 272, 7153, 1526, 1815, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [1, 422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 315, 506, 396, 10466, 1729, 5160, 2757, 28723, 28727, 494, 970, 315, 9951, 741, 2278, 3569, 1938, 264, 5283, 28723, 315, 682, 737, 298, 506, 264, 4241, 1203, 2342, 302, 272, 3036, 297, 264, 2245, 28723, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747], [1, 422, 320, 16804, 393, 8048, 422, 28747, 13, 3856, 4950, 2500, 28725, 4335, 1465, 28725, 6927, 3479, 1837, 28725, 22478, 1094, 1616, 2131, 28725, 1325, 740, 1249, 28725, 7379, 26802, 28725, 318, 308, 636, 24232, 472, 28725, 14319, 1098, 4950, 2500, 28725, 4625, 384, 22820, 28725, 9833, 4950, 2500, 28725, 9833, 28733, 532, 28733, 4176, 28725, 9833, 28733, 532, 28733, 1874, 28725, 7379, 28733, 532, 28733, 4176, 28725, 7379, 28733, 532, 28733, 11761, 28725, 24497, 22478, 1094, 1616, 2131, 28725, 14873, 22478, 1094, 1616, 2131, 28725, 9833, 9594, 466, 352, 28725, 3995, 362, 3978, 8258, 28725, 7379, 28733, 532, 28733, 24812, 5295, 28725, 15939, 1711, 8819, 5295, 3523, 3159, 685, 28725, 16957, 28733, 532, 28733, 13361, 28725, 16957, 4950, 2500, 28725, 9833, 2690, 4328, 13, 13, 28771, 15044, 1086, 422, 13, 12069, 2380, 272, 2188, 28742, 28713, 2159, 304, 8270, 3638, 5944, 304, 3638, 1304, 10001, 5246, 298, 12049, 378, 28723, 13, 13, 28771, 4515, 28824, 5057, 896, 7178, 422, 13, 28740, 28723, 415, 5032, 1580, 297, 264, 8113, 9292, 5032, 390, 9830, 5553, 28730, 16005, 1264, 733, 12230, 3707, 25308, 12052, 345, 5553, 28730, 12333, 1264, 733, 264, 1274, 302, 9796, 298, 347, 15731, 297, 7768, 298, 16427, 2188, 28742, 28713, 2159, 12052, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 5553, 1141, 613, 548, 345, 3731, 1264, 345, 5553, 1141, 461, 17395, 9205, 13, 28750, 28723, 415, 7138, 3638, 5944, 304, 3638, 9249, 541, 11024, 272, 2078, 2188, 2159, 9943, 28723, 10290, 1141, 1580, 347, 5937, 477, 320, 16804, 393, 8048, 28723, 13, 28770, 28723, 10290, 5944, 1023, 19470, 23581, 395, 3638, 9249, 28725, 304, 272, 1474, 302, 3638, 5944, 1023, 347, 1348, 395, 272, 3638, 9249, 28723, 13, 28781, 28723, 415, 3638, 9136, 1023, 7967, 272, 24940, 3352, 3638, 9249, 28725, 613, 28723, 28706, 28723, 272, 1745, 297, 690, 272, 10502, 2301, 460, 26377, 28723, 13, 13, 13, 28771, 2223, 725, 4515, 28824, 12171, 422, 28747, 5919, 18062, 653, 272, 2296, 2245, 304, 868, 4372, 272, 2996, 464, 3195, 349, 272, 2191, 9067, 302, 272, 2245, 3725, 1413, 272, 3857, 3469, 464, 7476, 28723, 11692, 4135, 7379, 28747, 464, 15183, 5168, 349, 264, 11108, 369, 877, 22433, 846, 18539, 304, 21256, 4994, 298, 8234, 18518, 298, 2822, 477, 304, 17700, 9796, 1059, 2659, 28725, 3210, 821, 312, 4089, 356, 9629, 16292, 28723, 851, 5514, 659, 18746, 11117, 298, 272, 4099, 302, 18278, 10895, 28725, 304, 378, 659, 750, 7589, 297, 4118, 5080, 1259, 390, 4229, 3842, 9457, 28725, 6074, 8021, 28725, 304, 8666, 13828, 1815, 13, 8479, 4665, 8270, 574, 1204, 297, 264, 8113, 9292, 5032, 28747, 13, 28771, 20308, 4874, 422, 28747]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"task_steps\": [\"Step 1: Translate the text description from English to French.\", \"Step 2: Edit the image according to the translated text description.\", \"Step 3: Perform image segmentation on the edited image.\"], \"task_nodes\": [\"Translation\", \"Image Editing\", \"Image Segmentation\"], \"task_links\": [{\"source\": \"Translation\", \"target\": \"Image Editing\"}, {\"source\": \"Image Editing\", \"target\": \"Image Segmentation\"}]}',\n",
       " '{\"task_steps\": [\"Step 1: Determine the similarity between two sentences\"], \"task_nodes\": [\"Sentence Similarity\"], \"task_links\": []}',\n",
       " '{\"task_steps\": [\"Step 1: Estimate the depth of objects in an image using Depth Estimation tool.\"], \"task_nodes\": [\"Depth Estimation\"], \"task_links\": []}',\n",
       " '{\"task_steps\": [\"Step 1: Use Text-to-Speech tool to convert the user-specified text into speech\"], \"task_nodes\": [\"Text-to-Speech\"], \"task_links\": []}',\n",
       " '{\"task_steps\": [\"Step 1: Convert the user provided audio file into text\"], \"task_nodes\": [\"Automatic Speech Recognition\"], \"task_links\": []}',\n",
       " '{\"task_steps\": [\"Step 1: Summarize the provided text.\", \"Step 2: Use the provided image and the summary text from Step 1 to answer the question.\"], \"task_nodes\": [\"Summarization\", \"Visual Question Answering\"], \"task_links\": [{\"source\": \"Summarization\", \"target\": \"Visual Question Answering\"}]}']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests = model.tokenizer(samples[\"request\"])\n",
    "print(requests)\n",
    "samples[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab5fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"task_steps\": [\"Step 1: Translate the text description from English to French.\", \"Step 2: Edit the image according to the translated text description.\", \"Step 3: Perform image segmentation on the edited image.\"], \"task_nodes\": [\"Translation\", \"Image Editing\", \"Image Segmentation\"], \"task_links\": [{\"source\": \"Translation\", \"target\": \"Image Editing\"}, {\"source\": \"Image Editing\", \"target\": \"Image Segmentation\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(samples[\"label\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23862565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 4335, 10020, 272, 2245, 5436, 477, 4300, 298, 4949, 9191, 345, 9977, 28705, 28750, 28747, 12838, 272, 3469, 4771, 298, 272, 19004, 2245, 5436, 9191, 345, 9977, 28705, 28770, 28747, 2744, 674, 3469, 10424, 352, 356, 272, 19527, 3469, 611, 1181, 345, 5553, 28730, 12333, 1264, 7367, 25825, 548, 345, 4176, 2690, 4328, 548, 345, 4176, 9594, 466, 352, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 25825, 548, 345, 3731, 1264, 345, 4176, 2690, 4328, 7706, 9830, 1394, 1264, 345, 4176, 2690, 4328, 548, 345, 3731, 1264, 345, 4176, 9594, 466, 352, 17395, 9205], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 5158, 21824, 272, 3684, 472, 1444, 989, 23748, 8883, 345, 5553, 28730, 12333, 1264, 7367, 26968, 636, 24232, 472, 8883, 345, 5553, 28730, 17052, 1264, 3980, 28752], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 3978, 3314, 272, 8478, 302, 6697, 297, 396, 3469, 1413, 3995, 362, 3978, 8258, 3921, 611, 1181, 345, 5553, 28730, 12333, 1264, 7367, 17603, 3978, 8258, 8883, 345, 5553, 28730, 17052, 1264, 3980, 28752], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 5938, 7379, 28733, 532, 28733, 24812, 5295, 3921, 298, 6603, 272, 2188, 28733, 4101, 1799, 2245, 778, 8666, 8883, 345, 5553, 28730, 12333, 1264, 7367, 1874, 28733, 532, 28733, 24812, 5295, 8883, 345, 5553, 28730, 17052, 1264, 3980, 28752], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 16186, 272, 2188, 3857, 10466, 1729, 778, 2245, 8883, 345, 5553, 28730, 12333, 1264, 7367, 23556, 1711, 8819, 5295, 3523, 3159, 685, 8883, 345, 5553, 28730, 17052, 1264, 3980, 28752], [9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 28747, 6927, 3479, 653, 272, 3857, 2245, 9191, 345, 9977, 28705, 28750, 28747, 5938, 272, 3857, 3469, 304, 272, 14060, 2245, 477, 7268, 28705, 28740, 298, 4372, 272, 2996, 611, 1181, 345, 5553, 28730, 12333, 1264, 7367, 9881, 3479, 1837, 548, 345, 21551, 22478, 1094, 1616, 2131, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 9881, 3479, 1837, 548, 345, 3731, 1264, 345, 21551, 22478, 1094, 1616, 2131, 17395, 9205]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "labels = model.tokenizer(samples[\"label\"], add_special_tokens=False)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "716e79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>[INST]'\n",
    "EOS_USER = '[/INST]'\n",
    "EOS = '</s>'\n",
    "\n",
    "IGNORE_INDEX = -100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7770031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_tokens = model.tokenizer(EOS, add_special_tokens=False)\n",
    "eos_user_tokens = model.tokenizer(EOS_USER, add_special_tokens=False)\n",
    "bos_embeds = model.word_embedding(model.tokenizer(BOS, add_special_tokens=False, return_tensors='pt').input_ids[0].to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4140bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [733, 28748, 16289, 28793], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_user_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3adcd6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3640e-03, -1.0633e-04, -5.6152e-03,  ..., -5.0545e-05,\n",
       "         -1.1520e-03,  1.5926e-04],\n",
       "        [-6.7139e-04, -5.7983e-04, -3.1891e-03,  ..., -1.7071e-04,\n",
       "          3.1281e-04,  8.5449e-04],\n",
       "        [ 1.4496e-04,  5.0354e-04, -2.3499e-03,  ..., -2.5024e-03,\n",
       "          3.2349e-03, -2.8229e-03],\n",
       "        [-4.1504e-03, -1.7548e-03,  3.7231e-03,  ..., -1.2589e-04,\n",
       "         -9.2697e-04,  3.2196e-03]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d4ba969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_embeds = model.word_embedding(torch.tensor(model.tokenizer.pad_token_id).to(model.device)).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e7fda47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1adc8ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0., 0., -0.,  ..., -0., -0., -0.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(pad_embeds)\n",
    "print(pad_embeds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11bd4872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d686cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(samples['id'])\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fdc3855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4096])\n"
     ]
    }
   ],
   "source": [
    "graph_embeds = model.encode_task_graph(task_graph, batch_size)\n",
    "print(graph_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a45990fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b197d661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0041], device='cuda:0', grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "graph_embeds = torch.mean(graph_embeds[0], dim=0, keepdim=True)\n",
    "print(graph_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d19a955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embed = graph_embeds.repeat(batch_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad8bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0806651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23fef6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0041],\n",
       "        [0.0041],\n",
       "        [0.0041],\n",
       "        [0.0041],\n",
       "        [0.0041],\n",
       "        [0.0041]], device='cuda:0', grad_fn=<RepeatBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb5f83cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41f29f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e734894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9830,\n",
       " 5553,\n",
       " 28730,\n",
       " 16005,\n",
       " 1264,\n",
       " 7367,\n",
       " 9977,\n",
       " 28705,\n",
       " 28740,\n",
       " 28747,\n",
       " 4335,\n",
       " 10020,\n",
       " 272,\n",
       " 2245,\n",
       " 5436,\n",
       " 477,\n",
       " 4300,\n",
       " 298,\n",
       " 4949,\n",
       " 9191,\n",
       " 345,\n",
       " 9977,\n",
       " 28705,\n",
       " 28750,\n",
       " 28747,\n",
       " 12838,\n",
       " 272,\n",
       " 3469,\n",
       " 4771,\n",
       " 298,\n",
       " 272,\n",
       " 19004,\n",
       " 2245,\n",
       " 5436,\n",
       " 9191,\n",
       " 345,\n",
       " 9977,\n",
       " 28705,\n",
       " 28770,\n",
       " 28747,\n",
       " 2744,\n",
       " 674,\n",
       " 3469,\n",
       " 10424,\n",
       " 352,\n",
       " 356,\n",
       " 272,\n",
       " 19527,\n",
       " 3469,\n",
       " 611,\n",
       " 1181,\n",
       " 345,\n",
       " 5553,\n",
       " 28730,\n",
       " 12333,\n",
       " 1264,\n",
       " 7367,\n",
       " 25825,\n",
       " 548,\n",
       " 345,\n",
       " 4176,\n",
       " 2690,\n",
       " 4328,\n",
       " 548,\n",
       " 345,\n",
       " 4176,\n",
       " 9594,\n",
       " 466,\n",
       " 352,\n",
       " 8883,\n",
       " 345,\n",
       " 5553,\n",
       " 28730,\n",
       " 17052,\n",
       " 1264,\n",
       " 733,\n",
       " 6799,\n",
       " 1394,\n",
       " 1264,\n",
       " 345,\n",
       " 25825,\n",
       " 548,\n",
       " 345,\n",
       " 3731,\n",
       " 1264,\n",
       " 345,\n",
       " 4176,\n",
       " 2690,\n",
       " 4328,\n",
       " 7706,\n",
       " 9830,\n",
       " 1394,\n",
       " 1264,\n",
       " 345,\n",
       " 4176,\n",
       " 2690,\n",
       " 4328,\n",
       " 548,\n",
       " 345,\n",
       " 3731,\n",
       " 1264,\n",
       " 345,\n",
       " 4176,\n",
       " 9594,\n",
       " 466,\n",
       " 352,\n",
       " 17395,\n",
       " 9205]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.input_ids[0][:model.max_new_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2430c530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels.input_ids[0][:model.max_new_tokens] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "339bcb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21ed3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9830,\n",
       " 5553,\n",
       " 28730,\n",
       " 16005,\n",
       " 1264,\n",
       " 7367,\n",
       " 9977,\n",
       " 28705,\n",
       " 28740,\n",
       " 5529,\n",
       " 2841,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 298,\n",
       " 1388,\n",
       " 272,\n",
       " 13355,\n",
       " 1369,\n",
       " 304,\n",
       " 3084,\n",
       " 21448,\n",
       " 1871,\n",
       " 28725,\n",
       " 3595,\n",
       " 28725,\n",
       " 304,\n",
       " 11487,\n",
       " 548,\n",
       " 345,\n",
       " 9977,\n",
       " 28705,\n",
       " 28750,\n",
       " 5529,\n",
       " 2841,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 298,\n",
       " 8270,\n",
       " 264,\n",
       " 13355,\n",
       " 2264,\n",
       " 2818,\n",
       " 356,\n",
       " 272,\n",
       " 3857,\n",
       " 1178,\n",
       " 548,\n",
       " 345,\n",
       " 9977,\n",
       " 28705,\n",
       " 28770,\n",
       " 5529,\n",
       " 15382,\n",
       " 28730,\n",
       " 7141,\n",
       " 17769,\n",
       " 298,\n",
       " 2623,\n",
       " 396,\n",
       " 15382,\n",
       " 14211,\n",
       " 288,\n",
       " 7103,\n",
       " 477,\n",
       " 23330,\n",
       " 904,\n",
       " 602,\n",
       " 4120,\n",
       " 298,\n",
       " 272,\n",
       " 16099,\n",
       " 302,\n",
       " 22830,\n",
       " 4120,\n",
       " 28725,\n",
       " 9720,\n",
       " 1059,\n",
       " 475,\n",
       " 742,\n",
       " 7645,\n",
       " 4120,\n",
       " 28725,\n",
       " 1312,\n",
       " 14517,\n",
       " 272,\n",
       " 25754,\n",
       " 7103,\n",
       " 304,\n",
       " 23329,\n",
       " 2948,\n",
       " 15014,\n",
       " 8883,\n",
       " 345,\n",
       " 5553,\n",
       " 28730,\n",
       " 12333,\n",
       " 1264,\n",
       " 7367,\n",
       " 14908,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 548,\n",
       " 345,\n",
       " 406,\n",
       " 9405,\n",
       " 28730,\n",
       " 7141,\n",
       " 17769,\n",
       " 8883,\n",
       " 345,\n",
       " 5553,\n",
       " 28730,\n",
       " 17052,\n",
       " 1264,\n",
       " 733,\n",
       " 6799,\n",
       " 1394,\n",
       " 1264,\n",
       " 345,\n",
       " 14908,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 548,\n",
       " 345,\n",
       " 3731,\n",
       " 1264,\n",
       " 345,\n",
       " 406,\n",
       " 9405,\n",
       " 28730,\n",
       " 7141,\n",
       " 17769,\n",
       " 17395,\n",
       " 9205,\n",
       " 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.input_ids[0][:model.max_new_tokens] + eos_tokens.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ace8e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_input_ids = labels.input_ids[0][:model.max_new_tokens] + eos_tokens.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8514dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 422,\n",
       " 320,\n",
       " 16804,\n",
       " 393,\n",
       " 8048,\n",
       " 422,\n",
       " 28747,\n",
       " 13,\n",
       " 2274,\n",
       " 28730,\n",
       " 7822,\n",
       " 288,\n",
       " 28725,\n",
       " 3472,\n",
       " 28730,\n",
       " 9157,\n",
       " 992,\n",
       " 28730,\n",
       " 14620,\n",
       " 28725,\n",
       " 4080,\n",
       " 28730,\n",
       " 28713,\n",
       " 1033,\n",
       " 28725,\n",
       " 2841,\n",
       " 28730,\n",
       " 25973,\n",
       " 2660,\n",
       " 28730,\n",
       " 1114,\n",
       " 282,\n",
       " 28730,\n",
       " 5134,\n",
       " 28725,\n",
       " 3472,\n",
       " 28730,\n",
       " 357,\n",
       " 9279,\n",
       " 28725,\n",
       " 8514,\n",
       " 28730,\n",
       " 5033,\n",
       " 28730,\n",
       " 3016,\n",
       " 28725,\n",
       " 8594,\n",
       " 28730,\n",
       " 20913,\n",
       " 28725,\n",
       " 10537,\n",
       " 28730,\n",
       " 262,\n",
       " 18831,\n",
       " 28725,\n",
       " 9442,\n",
       " 28730,\n",
       " 452,\n",
       " 7041,\n",
       " 28725,\n",
       " 14933,\n",
       " 4609,\n",
       " 28730,\n",
       " 9157,\n",
       " 992,\n",
       " 28730,\n",
       " 28717,\n",
       " 13759,\n",
       " 352,\n",
       " 28725,\n",
       " 9314,\n",
       " 28730,\n",
       " 2837,\n",
       " 28730,\n",
       " 7822,\n",
       " 263,\n",
       " 28725,\n",
       " 1220,\n",
       " 28730,\n",
       " 357,\n",
       " 9279,\n",
       " 28725,\n",
       " 7689,\n",
       " 28730,\n",
       " 3521,\n",
       " 288,\n",
       " 28730,\n",
       " 3385,\n",
       " 28725,\n",
       " 726,\n",
       " 28730,\n",
       " 20913,\n",
       " 28725,\n",
       " 4530,\n",
       " 28730,\n",
       " 11009,\n",
       " 28730,\n",
       " 19963,\n",
       " 28725,\n",
       " 882,\n",
       " 28730,\n",
       " 14556,\n",
       " 28725,\n",
       " 3472,\n",
       " 28730,\n",
       " 3290,\n",
       " 3507,\n",
       " 1549,\n",
       " 28725,\n",
       " 5835,\n",
       " 28730,\n",
       " 28707,\n",
       " 10106,\n",
       " 28730,\n",
       " 3385,\n",
       " 28725,\n",
       " 7223,\n",
       " 28730,\n",
       " 16714,\n",
       " 28730,\n",
       " 720,\n",
       " 4078,\n",
       " 28725,\n",
       " 6790,\n",
       " 28730,\n",
       " 20913,\n",
       " 28730,\n",
       " 9307,\n",
       " 28725,\n",
       " 2093,\n",
       " 28730,\n",
       " 24945,\n",
       " 28730,\n",
       " 2360,\n",
       " 28725,\n",
       " 3270,\n",
       " 28730,\n",
       " 9643,\n",
       " 466,\n",
       " 28730,\n",
       " 3521,\n",
       " 288,\n",
       " 28725,\n",
       " 11860,\n",
       " 28730,\n",
       " 19521,\n",
       " 28730,\n",
       " 2360,\n",
       " 28725,\n",
       " 1820,\n",
       " 28730,\n",
       " 1127,\n",
       " 7323,\n",
       " 28730,\n",
       " 3017,\n",
       " 28725,\n",
       " 9976,\n",
       " 28730,\n",
       " 1817,\n",
       " 28730,\n",
       " 3991,\n",
       " 28725,\n",
       " 7455,\n",
       " 28730,\n",
       " 9029,\n",
       " 28730,\n",
       " 5476,\n",
       " 28725,\n",
       " 1253,\n",
       " 28730,\n",
       " 1114,\n",
       " 282,\n",
       " 28730,\n",
       " 28717,\n",
       " 834,\n",
       " 3572,\n",
       " 28725,\n",
       " 1877,\n",
       " 28730,\n",
       " 1127,\n",
       " 7323,\n",
       " 28730,\n",
       " 3017,\n",
       " 28730,\n",
       " 494,\n",
       " 13011,\n",
       " 28725,\n",
       " 5835,\n",
       " 28730,\n",
       " 28707,\n",
       " 10106,\n",
       " 28730,\n",
       " 3521,\n",
       " 288,\n",
       " 28725,\n",
       " 20758,\n",
       " 28730,\n",
       " 7653,\n",
       " 288,\n",
       " 28725,\n",
       " 4530,\n",
       " 28730,\n",
       " 3063,\n",
       " 28730,\n",
       " 20913,\n",
       " 28725,\n",
       " 9506,\n",
       " 11332,\n",
       " 28730,\n",
       " 2888,\n",
       " 28730,\n",
       " 262,\n",
       " 28725,\n",
       " 2231,\n",
       " 28730,\n",
       " 7200,\n",
       " 28725,\n",
       " 2268,\n",
       " 28730,\n",
       " 7349,\n",
       " 28725,\n",
       " 5271,\n",
       " 1434,\n",
       " 361,\n",
       " 28730,\n",
       " 28720,\n",
       " 20569,\n",
       " 28725,\n",
       " 625,\n",
       " 28730,\n",
       " 6729,\n",
       " 28725,\n",
       " 5339,\n",
       " 28730,\n",
       " 13521,\n",
       " 28730,\n",
       " 10672,\n",
       " 301,\n",
       " 28725,\n",
       " 3472,\n",
       " 28730,\n",
       " 514,\n",
       " 2103,\n",
       " 28730,\n",
       " 14520,\n",
       " 28725,\n",
       " 1955,\n",
       " 28730,\n",
       " 28707,\n",
       " 5386,\n",
       " 28730,\n",
       " 28707,\n",
       " 10106,\n",
       " 28730,\n",
       " 2360]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.input_ids[0][:model.max_new_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7351036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(requests.input_ids[0][:model.max_new_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84cc0160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[733, 28748, 16289, 28793]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_user_tokens.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "037f0849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9830,\n",
       " 5553,\n",
       " 28730,\n",
       " 16005,\n",
       " 1264,\n",
       " 7367,\n",
       " 9977,\n",
       " 28705,\n",
       " 28740,\n",
       " 5529,\n",
       " 2841,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 298,\n",
       " 1388,\n",
       " 272,\n",
       " 13355,\n",
       " 1369,\n",
       " 304,\n",
       " 3084,\n",
       " 21448,\n",
       " 1871,\n",
       " 28725,\n",
       " 3595,\n",
       " 28725,\n",
       " 304,\n",
       " 11487,\n",
       " 548,\n",
       " 345,\n",
       " 9977,\n",
       " 28705,\n",
       " 28750,\n",
       " 5529,\n",
       " 2841,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 298,\n",
       " 8270,\n",
       " 264,\n",
       " 13355,\n",
       " 2264,\n",
       " 2818,\n",
       " 356,\n",
       " 272,\n",
       " 3857,\n",
       " 1178,\n",
       " 548,\n",
       " 345,\n",
       " 9977,\n",
       " 28705,\n",
       " 28770,\n",
       " 5529,\n",
       " 15382,\n",
       " 28730,\n",
       " 7141,\n",
       " 17769,\n",
       " 298,\n",
       " 2623,\n",
       " 396,\n",
       " 15382,\n",
       " 14211,\n",
       " 288,\n",
       " 7103,\n",
       " 477,\n",
       " 23330,\n",
       " 904,\n",
       " 602,\n",
       " 4120,\n",
       " 298,\n",
       " 272,\n",
       " 16099,\n",
       " 302,\n",
       " 22830,\n",
       " 4120,\n",
       " 28725,\n",
       " 9720,\n",
       " 1059,\n",
       " 475,\n",
       " 742,\n",
       " 7645,\n",
       " 4120,\n",
       " 28725,\n",
       " 1312,\n",
       " 14517,\n",
       " 272,\n",
       " 25754,\n",
       " 7103,\n",
       " 304,\n",
       " 23329,\n",
       " 2948,\n",
       " 15014,\n",
       " 8883,\n",
       " 345,\n",
       " 5553,\n",
       " 28730,\n",
       " 12333,\n",
       " 1264,\n",
       " 7367,\n",
       " 14908,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 548,\n",
       " 345,\n",
       " 406,\n",
       " 9405,\n",
       " 28730,\n",
       " 7141,\n",
       " 17769,\n",
       " 8883,\n",
       " 345,\n",
       " 5553,\n",
       " 28730,\n",
       " 17052,\n",
       " 1264,\n",
       " 733,\n",
       " 6799,\n",
       " 1394,\n",
       " 1264,\n",
       " 345,\n",
       " 14908,\n",
       " 28730,\n",
       " 9701,\n",
       " 2045,\n",
       " 28730,\n",
       " 1613,\n",
       " 548,\n",
       " 345,\n",
       " 3731,\n",
       " 1264,\n",
       " 345,\n",
       " 406,\n",
       " 9405,\n",
       " 28730,\n",
       " 7141,\n",
       " 17769,\n",
       " 17395,\n",
       " 9205,\n",
       " 2]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c476e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_requests + EOS_user + labels + EOS\n",
    "\n",
    "input_ids = requests.input_ids[0][:model.max_new_tokens] + eos_user_tokens.input_ids + label_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d228060e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d01f2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds = model.word_embedding(torch.tensor(input_ids).to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ab9b09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([407, 4096])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d886525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4096])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "324d26e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1cc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOS + graph_embed + user_requests + EOS_user + labels + EOS\n",
    "\n",
    "input_embeds = torch.cat([bos_embeds, graph_embeds[0].unsqueeze(0), input_embeds], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e664053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([412, 4096])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOS + graph_embed + user_requests + EOS_user + labels + EOS\n",
    "# 4   +       1         + 256         + 4       + 146    + 1\n",
    "\n",
    "input_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43fa796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs_embeds = []\n",
    "batch_attention_mask = []\n",
    "batch_label_input_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d26f80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs_embeds.append(input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bcd73295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "batch_attention_mask.append([1] * input_embeds.shape[0])\n",
    "print(len([1] * input_embeds.shape[0]))\n",
    "print([1] * input_embeds.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35c5668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_input_ids = [IGNORE_INDEX] * (input_embeds.shape[0] - len(label_input_ids)) + label_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "945dc49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9830, 5553, 28730, 16005, 1264, 7367, 9977, 28705, 28740, 5529, 2841, 28730, 9701, 2045, 28730, 1613, 298, 1388, 272, 13355, 1369, 304, 3084, 21448, 1871, 28725, 3595, 28725, 304, 11487, 548, 345, 9977, 28705, 28750, 5529, 2841, 28730, 9701, 2045, 28730, 1613, 298, 8270, 264, 13355, 2264, 2818, 356, 272, 3857, 1178, 548, 345, 9977, 28705, 28770, 5529, 15382, 28730, 7141, 17769, 298, 2623, 396, 15382, 14211, 288, 7103, 477, 23330, 904, 602, 4120, 298, 272, 16099, 302, 22830, 4120, 28725, 9720, 1059, 475, 742, 7645, 4120, 28725, 1312, 14517, 272, 25754, 7103, 304, 23329, 2948, 15014, 8883, 345, 5553, 28730, 12333, 1264, 7367, 14908, 28730, 9701, 2045, 28730, 1613, 548, 345, 406, 9405, 28730, 7141, 17769, 8883, 345, 5553, 28730, 17052, 1264, 733, 6799, 1394, 1264, 345, 14908, 28730, 9701, 2045, 28730, 1613, 548, 345, 3731, 1264, 345, 406, 9405, 28730, 7141, 17769, 17395, 9205, 2]\n"
     ]
    }
   ],
   "source": [
    "print(len(label_input_ids))\n",
    "print(label_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33cbebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_label_input_ids.append(label_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2c8a0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([x.shape[0] for x in batch_inputs_embeds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7e6322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d6c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11511d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777b80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306e339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab541a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e695f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c6c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e4ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1719cd2",
   "metadata": {},
   "source": [
    "## plan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45dc7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\\n\\n# GOAL #\\nPlease understand the user's request and generate task steps and task invocation graph to solve it.\"\"\" \\\n",
    "       + \"\"\"\\n\\n# REQUIREMENT #\\n1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\\n\"\"\" \\\n",
    "       + \"\"\"2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\\n\"\"\" \\\n",
    "       + \"\"\"3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\\n\"\"\" \\\n",
    "       + \"\"\"4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\\n\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "tool_list = json.load(open(f\"../data/{args.dataset}/tool_desc.json\", \"r\"))[\"nodes\"]\n",
    "\n",
    "tool_string = \"# TASK LIST #:\\n\" + \", \".join([task[\"id\"] for task in tool_list]) \n",
    "\n",
    "args.prompt = tool_string + PROMPT + \"\"\"\\n\\n# USER REQUEST #: {{user_request}}\\nNow please generate your result in a strict JSON format:\\n# RESULT #:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4394f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# TASK LIST #:\n",
      "order_tracking, search_repair_provider, send_sms, special_vehicle_rental_service, search_agenda, menu_select_api, manage_schedule, detailed_inquiry, schedule_planner, appliance_repair_cancellation, flight_status_tracker, read_agenda, hotel_booking_query, import_schedule, travel_plan_maker, del_transaction, search_restaurants, train_ticket_query, foreign_currency_exchange, daily_schedule_manager, product_catalog_search, online_appointment_booking, academic_paper_search, book_meeting_room, loan_info_entry, smart_home_control, car_rental_cancelling, check_meeting_room_availability, train_ticket_booking, agenda_sorting, travel_group_schedule, luggage_check_in, create_document, change_password, souvenir_purchase, get_menu, select_best_hotel, search_conference_rooms, business_trip_ticket_search, flight_info_query, checkout_api, traffic_card, website_design_tool, business_communication, cruise_ticket_search, cruise_ship_booking, souvenir_recommender, company_vehicle_service, create_meeting, restaurant_review, stock_market_trend, health_monitor_tool, search_transactions, check_room_availability, special_personality_test, outward_reception_approval, create_reminder, verify_file_content, conference_room_cancelling, theme_park_info_search, send_document, appliance_repair_status_query, visa_application, calculate_exchange_amount, food_recommendation, virtual_try_on_tool, check_balance, geolocation_tracker, weather_query, rename_file, send_email, file_modify, flight_ticket_cancelling, transfer_money, home_cleaning_booking, online_healing_session, visa_status_check, foreign_currency_sale, postal_code_search, home_cleaning_query, investment_portfolio_builder, flight_status_check, flight_ticket_changing, price_search, set_reminder, special_foreign_currency_purchase, clock_alarm_set, meeting_reservation, travel_insurance_search, sound_effects_tool, edit_agenda, clock_alarm_cancel, flight_status_search, payment_processing, theme_park_ticket_search, special_weather_forecast_tool, check_flight_seats_availability, part_time_job_tool, product_catalog_create, send_ticket, conference_room_query, travel_camera_suggestion, backup_agenda, airport_pickup, product_service_selection, home_cleaning_cancelling, car_rental_booking, file_compression, business_travel_standard_search, travel_journal, event_planning_tool, account_balance_query, appliance_repair_rescheduling, hotel_booking_modification, modify_transaction, route_planning, car_rental_query, table_booking_api, fixed_deposit_transaction, anime_tracking_tool, smart_appliance_control, virtual_classroom_reservation, get_repair_request_id, team_collaboration_tool, asset_checkout_approval, sleep_analysis_tool, open_icbc_messenger_service, parenting_advice_tool, check_file_existence, personal_tax_calculator, create_agenda, car_rental_changing, cash_withdrawal_reservation, internal_purchase, flight_search, notify_user, stock_trend_predictor, send_confirmation, calendar_note, custom_product_service_messenger, travel_camera_rental, login_gear, place_order, search_train, book_restaurant, time_tracker, get_recent_transaction, desktop_organizer, advance_ticket_booking, insurance_customer_update, price_comparison_tool, transport_card_recharge, account_login, insurance_claim_status, tour_group_search, send_business_message, insurance_product_search, credit_card_progress, business_trip_ticket_reservation, send_confirmation_message, news_headlines_generator, login_equipment_check, check_weather, confirm_appointment, calendar_annotation, check_flight_availability, book_flight, hotel_reservation_status, family_interaction_tool, send_meeting_invitations, home_cleaning_changing, merge_orders, file_write, geo_location_finder, theme_park_attractions_search, currency_exchange_rate, ai_search_tool, document_share_tool, create_schedule, check_room_booking_status, advanced_programming_lesson_planner, hotel_booking, internal_person_search, customer_service, network_speedup_tool, email_marketing_tool, clock_alarm_change, select_hotel, mortgage_calculator_tool, archive_file_tool, conference_room_booking, product_inventory_search, job_skills_lookup, b_b_availability_check, loan_application, music_playback, share_calendar, generate_meeting_alert, deposit_product_search, create_transaction, bank_balance_query, complaint_merchant, movie_recommendation_tool, tourist_souvenir_creator, login_system, pet_care_tool, outdoor_navigator, exchange_rate_converter, travel_itinerary_planner, sport_activity_recorder, travel_assistant, book_ticket, create_meeting_reminder, cruise_search, add_reminder, book_table, set_agenda_location, academic_achievement_query, transit_card_info, souvenir_search, travel_insurance_coverage, foreign_currency_purchase, credit_card_repayment, select_seats, foreign_currency_query, business_negotiation_tool, identity_verification, credit_card_debt, professional_photography_tips, deposit_product_selection, search_hotels, domestic_remittance, headline_news_search, file_delete, login_equipment, send_meeting_notification, travel_camera_info, get_dish_id, view_agenda, create_todo, update_contact_info, conference_room_changing, ssh_connect, scenic_spot_ticket_search, vehicle_tracker, train_ticket_changing, travelgroup_flight_search, account_security_check, appliance_repair_request, create_shared_calendar, digital_products_evaluation, search_authorized_service_provider, package_status_tracker, add_periodic_event, travel_diary_generator, account_logout, population_info_search, train_ticket_cancelling, currency_conversion, send_notifications\n",
      "\n",
      "# GOAL #\n",
      "Please understand the user's request and generate task steps and task invocation graph to solve it.\n",
      "\n",
      "# REQUIREMENT #\n",
      "1. The format must in a strict JSON format as {\"task_steps\": [ concrete step descriptions ], \"task_nodes\": [ a list of tasks to be executed in sequence to fulfill user's request ], \"task_links\": [{\"source\": \"task name i\", \"target\": \"task name j\"}]}\n",
      "2. The generated task steps and task nodes can resolve the given user request perfectly. Task name must be selected from TASK LIST.\n",
      "3. Task steps should strictly aligned with task nodes, and the number of task steps should be same with the task nodes.\n",
      "4. The task links should reflect the dependencies among task nodes, i.e. the order in which the APIs are invoked.\n",
      "\n",
      "\n",
      "# USER REQUEST #: {{user_request}}\n",
      "Now please generate your result in a strict JSON format:\n",
      "# RESULT #:\n"
     ]
    }
   ],
   "source": [
    "print(args.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f395ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f97895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "508378a4",
   "metadata": {},
   "source": [
    "## LLM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7472a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# llm_name = \"google/gemma-3-4B-it\"\n",
    "llm_name = \"google/gemma-3-270m-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513d5093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3TextConfig {\n",
       "  \"_sliding_window_pattern\": 6,\n",
       "  \"architectures\": [\n",
       "    \"Gemma3ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attn_logit_softcapping\": null,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"final_logit_softcapping\": null,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_size\": 640,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_types\": [\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"sliding_attention\",\n",
       "    \"full_attention\"\n",
       "  ],\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"gemma3_text\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"query_pre_attn_scalar\": 256,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_local_base_freq\": 10000.0,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": 512,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.55.4\",\n",
       "  \"use_bidirectional_attention\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 262144\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13464be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I'm a language model, I can generate text in many languages. However, I cannot truly *understand* the meaning of a word or\n",
      "> I'm a language model, and I don't have the ability to directly interact with the real world to experience the world in a way\n",
      "> I'm a language model, I don't have the capacity to interact with the world in a real-time way. However, I\n",
      "> I'm a language model, I cannot directly interact with the real world. I can only generate text based on the data I have been trained\n",
      "> I'm a language model, and I can generate text. I can write stories, poems, articles, and code. I am also able\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "seed=0\n",
    "model.eval()\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "prompt = \"I'm a language model,\"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "x = tokens\n",
    "x = x['input_ids']\n",
    "x = x.repeat(5,1)\n",
    "\n",
    "\n",
    "while x.size(1) < 30:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)['logits']\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "        \n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\">\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2602dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a552cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb74b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cb0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade223f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
